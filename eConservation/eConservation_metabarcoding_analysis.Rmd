---
title: " eConservation metabarcoding analysis"
author: 
  - "Hil√°rio, OH"
  - "Carvalho, DC"
date: "10/10/2023"
output: 
  html_document:
    code_download: yes
    df_print: paged
    keep_md: yes
    theme: flatly
    toc: true
    toc_depth: 5
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

#load project env and Rmd necessary libs
# load(file = "~/analises_eDNA/eConservation/ .RData")

```


 <font size="0.5">**This pipeline integrates public tools available for metagenomic analyses. To share or reproduce this content, please require authors' consent.**  
**Contact:** lgc.edna@gmail.com, heronoh@gmail.com</font> 

# Short introduction

  
Welcome! We will guide you trought the analysis of fish environmental eDNA from ichtyoplanckton samples from [eConservation](https://www.econservation.com.br/).

To proceed you will need the raw reads files and a .csv file with: 

a) a column listing all samples, with unique names identical to raw reads radicals; 

b) a column with the respective primer used for each sample; 

c) other columns for any metadata available.



# Bioinformatics

## Raw data aquisition

### Get reads from Base Sapce  

### Prepare analyses working directory

```{bash, eval=FALSE}
# 1 - creat a folder for all analyses files 

  #optionally, create the folders and paths as you want
# save project path into a bash variable
PRJCT_DIR=~/analises_eDNA/eConservation

# chech variable
echo $PRJCT_DIR;

# create folder
mkdir -p $PRJCT_DIR;

```

### Get reads from Base Sapce  

This part is only required if your read files is on Basespace. If the read files are already downloaded, proceed to the next section.

```{bash, eval=FALSE, echo=TRUE}
# 1 - create and navigate to run files folder
mkdir ~/analises_eDNA/eConservation;

cd  ~/analises_eDNA/eConservation;

# 2 - download run from BaseSpace
# 2a - autenticate to basespace (must have a basespace account and shared projects/runs)
bs auth;

# 2b - list your projects data
bs list datasets;

# 2c - download demultiplexed read files
bs download project -n "eConservation" -o fastq --extension=fastq.gz


# 3 - organize raw data
#go to reads directory
cd /data/data_raw/eConservation;

#create folder for raw files
mkdir raw;

# store raw data path into variable
RAW_DATA=~/analises_eDNA/eConservation

#copy/move all fastqs to raw folder */*
#     possibilita a copia dos arquivos com nome comecando com EM118 e terminando com gz para a pasta raw a aprtir da pasta fastq

mv $RAW_DATA/fastq/EM123_*/*gz $RAW_DATA/raw; 

ls $RAW_DATA/raw; 

```

### Quality checking

```{bash, eval=FALSE}
# 1 - creat folder for quality checking files

mkdir $PRJCT_DIR/quality;

# 2 - run FASTqc on all reads 

ls $RAW_DATA/raw/*

#roda um por vez
#fastqc $RAW_DATA/* --outdir $PRJCT_DIR/quality

#roda todos os arquivos em paralelo
find $RAW_DATA/raw -name '*.fastq.gz' 2>/dev/null | parallel fastqc {1} -o $PRJCT_DIR/quality/

# 3 - run MULTIqc on FASTqc files to integrate results
## erro no python

mkdir $PRJCT_DIR/quality/multiqc 
multiqc --interactive $PRJCT_DIR/quality --outdir $PRJCT_DIR/quality/multiqc 

# 4 - navigate on the files Rstudio pannel to open MULTIqc report on web browser and view results
```


## Raw data preprocessing

Now with quality assessment done, we will proceed into quality filtering, sample definition, primer removal and other steps. The main package used is DADA2 (https://doi.org/10.1038/nmeth.3869).

### DADA2

#### Load  R libs

```{r, eval=FALSE,echo=TRUE}
# 0 - load libraries and other programs ----
{
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(stringr)
  library(ggplot2)
  library(ggbreak)
  library(phyloseq)
  library(Biostrings)
  library(Matrix)
  library(ShortRead)
  library(dada2)
  library(DECIPHER)
  library(future)
  library(ggh4x)
  library(vegan)
  
}

# complete path to cutadapt executable
cutadapt <- "/usr/local/bin/cutadapt"
```

<br>

#### Set output and data paths

Here we will define a single project folder, and the pipeline will create the necessary subfolders for results organization.
Only the this main project folder has to be edited on the code bellow.


```{r, eval=FALSE,echo=TRUE}
# 1 - create and set output and input paths ----

  # use the same path you created on the bash $PRJCT_DIR variable
  analysis_path <- "~/analises_eDNA/eConservation"


# This block is automated and can be executed alone, before the first '{'

{  
    
  #project name radical
  prjct_rad <-c("eConservation")
  
  
  # create data_folder
  data_path <- paste0(analysis_path,"/data")
  if(!dir.exists(data_path)){ 
    dir.create(data_path)
  }else{
      print(paste0("The folder data_path already exists"))
    }
  
  # create processed reads folder
  pipe_libs <- paste0(data_path,"/reads")
  if(!dir.exists(pipe_libs)){ 
    dir.create(pipe_libs)
  }else{
      print(paste0("The folder pipe_libs already exists"))
    }
  
  # create processed reads folder
  raw_libs <- paste0(data_path,"/reads/raw")
  if(!dir.exists(raw_libs)){ 
    dir.create(raw_libs)
  }else{
      print(paste0("The folder raw_libs already exists"))
    }
  
  # create processed reads folder
  cutadapt_libs <- paste0(data_path,"/reads/cutadapt")
  if(!dir.exists(cutadapt_libs)){ 
    dir.create(cutadapt_libs)
  }else{
      print(paste0("The folder cutadapt_libs already exists"))
    }
  
  # create results folder
  results_path <- paste0(analysis_path,"/results")
  if(!dir.exists(results_path)){ 
    dir.create(results_path)
  }else{
      print(paste0("The folder results_path already exists"))
    }
  
  # create figs folder
  figs_path <- paste0(results_path,"/figs")
  if(!dir.exists(figs_path)){ 
    dir.create(figs_path)
  }else{
      print(paste0("The folder figs_path already exists"))
    }
  
  # create blast folder
  blast_path <- paste0(results_path,"/blast")
  if(!dir.exists(blast_path)){ 
    dir.create(blast_path)
  }else{
      print(paste0("The folder blast_path already exists"))
    }
  
  # create swarm folder
  swarm_path <- paste0(results_path,"/swarm")
  if(!dir.exists(swarm_path)){ 
    dir.create(swarm_path)
  }else{
      print(paste0("The folder swarm_path already exists"))
    }

}

list.files(analysis_path)

```





## Check samples

```{r, eval=FALSE,echo=TRUE}
# load primers indexes and samples table

## This is the most impotant input on the anlaysis
### Required columns with unique values: File_name, Primer FWD_sequence, Primer REV_sequence

#### This table was edited on https://docs.google.com/spreadsheets/d/1KnJOHQiDOGlJqXpxUwqpI67WV_zRuZNg/edit#gid=1273543466 on the tab with same date label

primers_n_samples <- readr::read_csv(file = "~/analises_eDNA/eConservation/data/eConservation-primers_n_samples.csv") %>% 
  # unite(col = "Unique_File_name", 
  #       File_name, Primer, 
  #       remove = F, sep = "_") %>% 
  mutate("Unique_File_name" = File_name) %>% 
  dplyr::rename("Metadata 1" = "metadata_1",
                "Metadata 2" = "metadata_2",
                "Metadata 3" = "metadata_3",
                "Metadata 4" = "metadata_4",
                "Metadata 5" = "metadata_5",
                "Metadata 6" = "metadata_6"
                ) %>% 
  relocate(Unique_File_name)

  # filter(!File_name %in% c("Nada")) 
  

# creating places to store raw data path ----
primers_n_samples <- primers_n_samples %>% 
  as_tibble() %>% 
  mutate("Lib name F" = "Lib name F",
         "Lib name R" = "Lib name R"  ) 

# chech if is there any duplicated file name ----
primers_n_samples$Unique_File_name %>% length()
primers_n_samples$Unique_File_name %>% unique() %>% length()
primers_n_samples$Unique_File_name %>% duplicated() %>% which()
primers_n_samples$Unique_File_name[primers_n_samples$Unique_File_name %>% duplicated()]



#recover file name radicals for linking raw files to samples ----


for (line in 1:nrow(primers_n_samples)) {

  if (primers_n_samples$Demultiplexed[line] == TRUE) {
    primers_n_samples$`Lib name F`[line] <- paste0(primers_n_samples$`Primer FWD_name`[line],"-",primers_n_samples$`Primer REV_name`[line])
    primers_n_samples$`Lib name R`[line] <- paste0(primers_n_samples$`Primer REV_name`[line],"-",primers_n_samples$`Primer FWD_name`[line])
  }else {
    # primers_n_samples$`Lib name F`[line] <- NA
    primers_n_samples$`Lib name F`[line] <- primers_n_samples$File_name[line]
  }
}


# chech if is there any duplicated tag pair ----
dplyr::select(primers_n_samples,c("Primer FWD_name","Primer REV_name","Lib")) %>% unite(col = "col",sep = "-") 
dplyr::select(primers_n_samples,c("Primer FWD_name","Primer REV_name","Lib")) %>% unite(col = "col",sep = "-") %>% nrow()
dplyr::select(primers_n_samples,c("Primer FWD_name","Primer REV_name","Lib")) %>% unite(col = "col",sep = "-") %>% unique() %>% nrow()
dplyr::select(primers_n_samples,c("Primer FWD_name","Primer REV_name","Lib")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()
dplyr::select(primers_n_samples,c("Primer FWD_name","Primer REV_name","Lib")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()


primers_n_samples$File_name[select(primers_n_samples,c("Primer FWD_name","Primer REV_name")) %>% unite(col = "col",sep = "-") %>% duplicated() %>% which()]


```



## Repairing

To avoid merging reads that did no come from the same cluster, we must check read pairing and remove unpaired (mandatory on DADA2).

```{r, eval=FALSE,echo=TRUE}

#list files
# colnames(primers_n_samples)[colnames(primers_n_samples) == "Paired data path"] <- "Raw data path"
all_fnFs <- sort(list.files((primers_n_samples$`Raw data path` %>%  unique()), 
                            pattern="_R1_001.fastq|\\.1.fastq|\\.R1.fastq", full.names = TRUE))
all_fnRs <- sort(list.files((primers_n_samples$`Raw data path` %>%  unique()), 
                            pattern = "_R2_001.fastq|\\.2.fastq|\\.R2.fastq", full.names = TRUE))

length(all_fnFs)
length(all_fnRs)

#6- loading sample data (origin and indexes)

colnames(primers_n_samples)

sample_idx_tbl <- primers_n_samples %>% dplyr::select(c("Unique_File_name", "File_name", "Lib", 
                                                 # "Primer FWD_sequence", "Primer REV_sequence",
                                                 "Project","Cliente", "Sample",
                                                 "Primer FWD_name","Primer REV_name",
                                                 "Demultiplexed","Raw data path","Primer",
                                                 "Lib name F", "Lib name R",
                                                 starts_with("Metadata"),
                                                 ends_with("control") ))


sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1" = "F-R1",
         "FWD_R2" = "F-R2",
         "REV_R1" = "R-R1",
         "REV_R2" = "R-R2")


# for both demultiplexed or not ----
for (sample in 1:nrow(sample_idx_tbl)) {

  print(sample_idx_tbl$File_name[sample])
  
  if (sample_idx_tbl$Demultiplexed[sample] == FALSE) {
    
    
    sample_idx_tbl$FWD_R1[sample] <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],""),x = all_fnFs)]
    
    sample_idx_tbl$FWD_R2[sample] <-  all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],""),x = all_fnRs)]

  }else{
    
    
    print(all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name R`[sample],"\\.R"),x = all_fnFs)])
    
    #FWD oriented amplicon
    
  if(!identical(character(0),all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,
                                      x = all_fnFs)])){
  sample_idx_tbl$FWD_R1[sample]  <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,
                                      x = all_fnFs)]
  }

    
    
  if(!identical(character(0),all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,
                                      x = all_fnRs)])){
  sample_idx_tbl$FWD_R2[sample]  <- all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,
                                      x = all_fnRs)]
  }

    #REV oriented amplicon
    
  if(!identical(character(0),all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,
                                      x = all_fnFs)])){
  sample_idx_tbl$REV_R1[sample]  <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,
                                      x = all_fnFs)]
  }

    
    
  if(!identical(character(0),all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,
                                      x = all_fnRs)])){
  sample_idx_tbl$REV_R2[sample]  <- all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                     "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,
                                      x = all_fnRs)]
  }
  }
}
# 

# create file paths for paired reads ----

sample_idx_tbl <- sample_idx_tbl %>%
  mutate("FWD_R1_paired" = "fwd_R1_paired",
         "FWD_R2_paired" = "fwd_R2_paired",
         "REV_R1_paired" = "rev_R1_paired",
         "REV_R2_paired" = "rev_R2_paired")



#create dir for recovered paired reads
 dir.create(path = paste0(pipe_libs,"/paired"),showWarnings = TRUE) 
 paired_libs <- paste0(pipe_libs,"/paired")
 
 
# Used for both demultiplexes and not, changing file names form tags to File_name radical

for (sample in 1:nrow(sample_idx_tbl)) {

  if (sample_idx_tbl$Demultiplexed[sample] == FALSE) { 
    sample_idx_tbl$FWD_R1_paired[sample] <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],""),
                                      fixed = T,x = all_fnFs)] %>%
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name F`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-FWD_R1") )

    sample_idx_tbl$FWD_R2_paired[sample] <- all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],""),
                                      fixed = T,x = all_fnRs)] %>% 
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name F`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-FWD_R2") )


  }else{

    #FWD amplicon
    if(!identical(character(0),all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,x = all_fnFs)])){
    sample_idx_tbl$FWD_R1_paired[sample] <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,x = all_fnFs)] %>% 
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name F`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-FWD_R1") )
    }

    if(!identical(character(0),all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,x = all_fnRs)])){
    sample_idx_tbl$FWD_R2_paired[sample] <- all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name F`[sample],".R"),
                                      fixed = T,x = all_fnRs)] %>% 
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name F`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-FWD_R2") )
    }

    #REV amplicon
    if(!identical(character(0),all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,x = all_fnFs)])){
    sample_idx_tbl$REV_R1_paired[sample] <- all_fnFs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,x = all_fnFs)] %>% 
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name R`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-REV_R1") )
    }

    if(!identical(character(0),all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,x = all_fnRs)])){
    sample_idx_tbl$REV_R2_paired[sample] <- all_fnRs[grep(pattern =  paste0(sample_idx_tbl$`Raw data path`[sample],
                                                                            "/",sample_idx_tbl$`Lib name R`[sample],".R"),
                                      fixed = T,x = all_fnRs)] %>% 
      str_replace(pattern = sample_idx_tbl$`Raw data path`[sample],
                  replacement = paired_libs) %>% 
      str_replace(pattern = sample_idx_tbl$`Lib name R`[sample],
                  replacement = paste0(sample_idx_tbl$Unique_File_name[sample],"-REV_R2") )
    }

  }
}


#naming read files with sample names ----
 # OBS: FWD and REV amplicons have the same File_name (DANGER!!)
{
  names(sample_idx_tbl$FWD_R1) <- paste0(sample_idx_tbl$Unique_File_name,"-","FWD")
  names(sample_idx_tbl$FWD_R2) <- paste0(sample_idx_tbl$Unique_File_name,"-","FWD")
  names(sample_idx_tbl$REV_R1) <- paste0(sample_idx_tbl$Unique_File_name,"-","REV")
  names(sample_idx_tbl$REV_R2) <- paste0(sample_idx_tbl$Unique_File_name,"-","REV")
  names(sample_idx_tbl$FWD_R1_paired) <- paste0(sample_idx_tbl$Unique_File_name,"-","FWD")
  names(sample_idx_tbl$FWD_R2_paired) <- paste0(sample_idx_tbl$Unique_File_name,"-","FWD")
  names(sample_idx_tbl$REV_R1_paired) <- paste0(sample_idx_tbl$Unique_File_name,"-","REV")
  names(sample_idx_tbl$REV_R2_paired) <- paste0(sample_idx_tbl$Unique_File_name,"-","REV")
  names(sample_idx_tbl$`Raw data path`) <- sample_idx_tbl$Unique_File_name
}



reads_fnFs <- c(sample_idx_tbl$FWD_R1,sample_idx_tbl$REV_R1)
reads_fnRs <- c(sample_idx_tbl$FWD_R2,sample_idx_tbl$REV_R2)
reads_fnFs_paired <- c(sample_idx_tbl$FWD_R1_paired,sample_idx_tbl$REV_R1_paired)
reads_fnRs_paired <- c(sample_idx_tbl$FWD_R2_paired,sample_idx_tbl$REV_R2_paired)





#remove unexisting paths (from REV amplicons)

reads_fnFs <- reads_fnFs[!reads_fnFs %in% c("R-R1","rev_R1_paired","R-R2","rev_R2_paired")]
reads_fnRs <- reads_fnRs[!reads_fnRs %in% c("R-R1","rev_R1_paired","R-R2","rev_R2_paired")]
reads_fnFs_paired <- reads_fnFs_paired[!reads_fnFs_paired %in% c("R-R1","rev_R1_paired","R-R2","rev_R2_paired")]
reads_fnRs_paired <- reads_fnRs_paired[!reads_fnRs_paired %in% c("R-R1","rev_R1_paired","R-R2","rev_R2_paired")]


#check vectors length

length(reads_fnFs)
length(reads_fnRs)
length(reads_fnFs_paired)
length(reads_fnRs_paired)


names(reads_fnFs)
names(reads_fnRs)
names(reads_fnFs_paired)
names(reads_fnRs_paired)

reads_fnFs[6]
reads_fnRs[6]
reads_fnFs_paired[6]
reads_fnRs_paired[6]

reads_fnFs %>% file.exists()
reads_fnFs_paired %>% file.exists()
reads_fnRs %>% file.exists()
reads_fnRs_paired %>% file.exists()


reads_fnFs[!reads_fnFs %>% file.exists()]
reads_fnRs[!reads_fnRs %>% file.exists()]

# Repairing in parallell ----


##############Function to repair and filter reads after demultiplexing (from CDI or not)
repairNfilt_with_DADA2 <- function(Unique_File_name,
                                   FWD_R1,
                                   FWD_R2,
                                   FWD_R1_paired,
                                   FWD_R2_paired,
                                   REV_R1,
                                   REV_R2,
                                   REV_R1_paired,
                                   REV_R2_paired){
  #FWD oriented amplicon
  all_filtered_out <- as_tibble()
  if(file.exists(FWD_R1)){

    print(paste0("Working on file: ",sample_idx_tbl$Unique_File_name))
    print(paste0("      FWD orientation"))

    sample_filtered_out_FWD <- as_tibble()

    sample_filtered_out_FWD <- dada2::filterAndTrim(
      fwd = FWD_R1,
      filt = FWD_R1_paired,
      rev = FWD_R2,
      filt.rev = FWD_R2_paired,
      maxN = c(0,0),
      maxEE = c(30,30),
      multithread = T,
      matchIDs = TRUE,
      rm.phix = TRUE,
      compress = TRUE,
      minLen = 30,
      verbose = TRUE)

    sample_filtered_out_FWD <- sample_filtered_out_FWD %>%
      as_tibble() %>%
      mutate("Unique_File_name" = Unique_File_name,
             "Orientation" = "FWD")
    }

#REV oriented amplicon
  if(file.exists(REV_R1)){

    print(paste0("      REV orientation"))

    sample_filtered_out_REV <- as_tibble()

    sample_filtered_out_REV <- dada2::filterAndTrim(
      fwd = REV_R1,
      filt = REV_R1_paired,
      rev = REV_R2,
      filt.rev = REV_R2_paired,
      maxN = c(0,0),
      maxEE = c(30,30),
      multithread = T,
      matchIDs = TRUE,
      rm.phix = TRUE,
      compress = TRUE,
      minLen = 30,
      verbose = TRUE)

    sample_filtered_out_REV <- sample_filtered_out_REV %>%
      as_tibble() %>%
      mutate("Unique_File_name" = Unique_File_name,
             "Orientation" = "REV")
    
    
    all_filtered_out <- bind_rows(all_filtered_out,sample_filtered_out_FWD,sample_filtered_out_REV)
    

  }else{

    print(paste0("-------- it has no REV oriented files"))

  }

  all_filtered_out <- bind_rows(all_filtered_out,sample_filtered_out_FWD)
  
  return(all_filtered_out)
}

all_filtered_out


#using the function ----

# Vers√µes paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))


tictoc::tic()
all_filtered_out_fun <- repairNfilt_with_DADA2(Unique_File_name = sample_idx_tbl$Unique_File_name,
                                              FWD_R1 = sample_idx_tbl$FWD_R1,
                                              FWD_R2 = sample_idx_tbl$FWD_R2,
                                              FWD_R1_paired = sample_idx_tbl$FWD_R1_paired,
                                              FWD_R2_paired = sample_idx_tbl$FWD_R2_paired,
                                              REV_R1 = sample_idx_tbl$REV_R1,
                                              REV_R2 = sample_idx_tbl$REV_R2,
                                              REV_R1_paired = sample_idx_tbl$REV_R1_paired,
                                              REV_R2_paired = sample_idx_tbl$REV_R2_paired)


tictoc::toc()


# calculating proportions ----


all_filtered_out <- all_filtered_out_fun %>% 
  mutate(prop = round((reads.out/reads.in*100),digits = 2)) %>% 
  dplyr::rename("Raw reads" = "reads.in",
         "Paired reads"  = "reads.out",
          "Proportion" = "prop")


all_filtered_out$Unique_File_name %>% unique()


all_filtered_out_wide <- all_filtered_out %>% pivot_wider(id_cols = Unique_File_name,
                                                          names_from = Orientation,
                                                          values_from = c(`Raw reads`, `Paired reads`,Proportion), values_fn = list) %>% 
  unnest(cols = everything() ) %>% 
  unique()

all_filtered_out_wide$Unique_File_name %>% unique()




all_filtered_out_wide$Unique_File_name[!all_filtered_out_wide$Unique_File_name %in%( all_filtered_out$Unique_File_name %>% unique())]

all_filtered_out_wide$Unique_File_name[all_filtered_out_wide$Unique_File_name %>% duplicated()]



base::save.image(paste0(analysis_path,"/",prjct_rad,"-env_",Sys.Date(),"_repaired.RData"))


```
<br>


### Identify file name names radicals

Here we set the raw reads files  and the .csv table containing the informations about sample name, primers, indexes, controls and any other metadata, such as local of collection, sampling dates, replicates, volume, weather conditions, etc. 

This table must have the columns **Sample**, **Primer** and **Unique_File_name**. This last one must identify uniquely the samples, withe the prefix radicals correspondent to their respective R1 and R2 read files.

```{r, eval=FALSE,echo=TRUE}
primers_n_samples$Unique_File_name %>% paste0(collapse = '","') %>% cat()
primers_n_samples$Unique_File_name %>% paste0(collapse = '","') %>% cat()

#organize sample names
{
sample_levels <- c( primers_n_samples$Unique_File_name)

}

```

<br>

### 

We will now create, from the samples table, another table to organize reads files and samples, with columns pointing to the reads files at every step of quality control and filtering. 

```{r, eval=FALSE,echo=TRUE}

# 3 - Map sample names to reads files ----


primers_n_samples$Unique_File_name %>% duplicated() %>% sum()


sample_idx_tbl %>% colnames()


sample_idx_tbl_wide <- sample_idx_tbl

sample_idx_tbl <- sample_idx_tbl_wide %>% 
  pivot_longer(cols = c(
    "FWD_R1", "FWD_R2", "REV_R1", "REV_R2", "FWD_R1_paired", "FWD_R2_paired", "REV_R1_paired", "REV_R2_paired"),
               names_to = "Stage",
               values_to = "Read file")



sample_idx_tbl$`Read file` %>% unique()

```

<br>

#### Identify primers on the original sequences

```{r, eval=FALSE,echo=TRUE}
#1 - identify primers ----

sample_idx_tbl$Primer %>% unique()

#primers sequences used for each sample

#              inosine pairs with A, C, U
#                               T, G, A = IUPAC code:  D
#              cutadapt  accepts IUPAC code !!!!!!!!
#        https://www.bioinformatics.org/sms/iupac.html

# Name primers: XXXxxXXXX_FWD or XXXxx-XXXX_REV

{
# #Fish1 ----
Fish1_F <- "TCAACCAACCACAAAGACATTGGCAC"
names(Fish1_F) <- "Fish1_F"
Fish1_R <- "TAGACTTCTGGGTGGCCAAAGAATCA"
names(Fish1_R) <- "Fish1_R"
# # 
# #Fish2 ----
Fish2_F <- "TCGACTAATCATAAAGATATCGGCAC"
names(Fish2_F) <- "Fish2_F"
Fish2_R <- "ACTTCAGGGTGACCGAAGAATCAGAA"
names(Fish2_R) <- "Fish2_R"
# # 
# # #VF2_FR1d ----
VF2_FR1d_F  <- "CAACCAACCACAAAGACATTGGCAC"
names(VF2_FR1d_F) <- "VF2_FR1d_F"
VF2_FR1d_R <- "ACCTCAGGGTGTCCGAARAAYCARAA"
names(VF2_FR1d_R) <- "VF2_FR1d_R"


 # creates a list of single row tibbles for each primer ----
primers <- tibble("Primer seq" = c(
  Fish1_F, Fish1_R,
  Fish2_F, Fish2_R,
  VF2_FR1d_F, VF2_FR1d_R
)) %>% 
  mutate(`Primer name`= names(`Primer seq`)) %>% 
  split(1:nrow(.)) 
}


primers
```

<br>

#### Generate sequences for complement, reverse and reverse complement of each primer

The function _allOrients_ is used to generate all possible orientations for primers FWD e REV.

```{r eval=FALSE,echo=TRUE}
# 1 - generate all possible primer orientations ----

#function to get all possible primer orientations ----
allOrients <- function(primers) {
   # Create all orientations of the input sequence
    # Must be a tibble with cols = c(Primers,`Primer name`)
  
   require(Biostrings)
   dna <- Biostrings::DNAString(primers$`Primer seq`)  # The Biostrings works w/ DNAString objects rather than character vectors
   orients <- c(Forward = dna, 
                Complement = Biostrings::complement(dna), 
                Reverse = Biostrings::reverse(dna),
                RevComp = Biostrings::reverseComplement(dna))
   names(orients) <- paste0(names(orients))
   
   primer_tbl <- sapply(orients, toString)
   
   primer_tbl <- tibble(Sequence = primer_tbl,
                        `Primer orientation` = names(primer_tbl)) %>% 
     dplyr::mutate(`Primer` = primers$`Primer name`) %>%
     tidyr::unite(col=`Orientation name`, `Primer` ,`Primer orientation`,remove = FALSE)
   
   # %>% 
   #   tidyr::pivot_wider( names_from = `Primer orientation`,values_from = Sequence)
   
   return(primer_tbl)  # Convert back to character vector
}



# 2 - Apply function to generate table with all primers orientations possible
primers_all_orients <- purrr::map_dfr(primers, allOrients)

#name the sequences accordingly
names(primers_all_orients$Sequence) <- primers_all_orients$`Orientation name`



primers_all_orients <- primers_all_orients %>% mutate(`Primer pair` = str_remove_all(string = Primer,
                                                              pattern = "_.*.$"))

#check naming
primers_all_orients$Sequence
```






<br>

#### Count primer presence on reads

Before primer removal it is possible to count their presence on the reads. This procedures is carried on independently for each sample. The following example applies to the first samples of each primer sample set.

```{r eval=FALSE}
# 1 - prepare to count primer orientation hits ----

# 1a - Load required functions ----
#function to count primer on each specific library
primerHits <- function(primer, fn) {
   # Counts number of reads in which the primer is found
   nhits <- Biostrings::vcountPattern(primer, ShortRead::sread(ShortRead::readFastq(fn)), fixed = FALSE,
                                      max.mismatch = 1)
   return(sum(nhits > 0))
}

#function to call primerHits for multiple primers
multi_primerHits <- function(Read_file,primers){
  primer_counts <- purrr::map_df(primers,.f = primerHits, 
                                 fn = Read_file)
  primer_counts <- primer_counts %>%  mutate(`Read file` = Read_file)
  return(primer_counts)
}


sample_idx_tbl$Stage %>% unique()


# 2b - Create vector of read files to look on for primers ----
reads_seqs <- sample_idx_tbl %>% 
  filter(Stage %in% c("FWD_R1_paired", "FWD_R2_paired")) %>% 
  dplyr::select(`Read file`) %>% as.list()


reads_seqs %>% lengths()


# 2c - named vector of primer sequences ----
primers_seqs <- primers_all_orients$Sequence

# 2d - Set up for parallel searching ----
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78

future::plan(future::multisession(workers = cores_to_be_used))

# 3 - Count primers on reads ----
primers_in_Nreads <- furrr::future_map_dfr(reads_seqs$`Read file`, .f = multi_primerHits, primers = primers_seqs, .options = furrr::furrr_options(seed = NULL))

# 4 - identify which primers were found on most reeads ----
dim(primers_in_Nreads)

colSums(primers_in_Nreads[,1:length(primers_seqs)]) %>% sort()

# 5 - Save primers in N-cleaned-reads complete table (if desired)

write.csv(x = primers_in_Nreads, file = paste0(results_path,"/",prjct_rad,"-primers_found_in_reads.csv"))

# 6- Remove columns(primers) not found in any sample

#Identify empty counts
colnames(primers_in_Nreads) #choose only numeric columns (primer counts)
colnames(primers_in_Nreads[1:length(primers_seqs)]) #choose only numeric columns (primer counts)
colSums(primers_in_Nreads[1:length(primers_seqs)]) # d√° pra excluir do plot se tiver zero counts
(colSums(primers_in_Nreads[1:length(primers_seqs)]) == 0)#transformando em um vetor logico
colnames(primers_in_Nreads)[(colSums(primers_in_Nreads[1:length(primers_seqs)]) != 0)]#transformando em um vetor logico


# primers_in_Nreads <- primers_in_Nreads[,c((colSums(primers_in_Nreads[1:72]) != 0),TRUE),] # d√° pra excluir do plot se tiver zero counts

colnames(primers_in_Nreads)

#get sample information into primers_in_Nreads table
primers_in_Nreads <- left_join(primers_in_Nreads,sample_idx_tbl,by = "Read file") %>% 
  mutate(Read = if_else((str_detect(Stage,
                                    pattern = "R1")),
                        "R1",
                        "R2")) %>% 
  mutate(Unique_File_name = factor(Unique_File_name,levels = sample_levels)) %>%
  unite(col = "Unique_File_name_Read", sep = " ", remove = FALSE,
        Unique_File_name,Read) 


# count numbers of reads in original RAW files

primers_in_Nreads <- primers_in_Nreads %>% 
  mutate(`Total reads` =  ShortRead::countFastq(dirPath = .$`Read file`)[,1])


#7- prepare primer counts for plots ----

rownames(primers_in_Nreads) <- primers_in_Nreads$Unique_File_name_Read

primers_in_Nreads$`Read file`

```

#### Plot primers identified in each library

```{r, echo=TRUE,eval=FALSE}
#8 - prepare primer counts for plots in ggplot----

primers_all_orients$`Orientation name`


primers_in_Nreads %>% colnames() %>%  paste0(collapse = '",\n"') %>% cat()

#convert primer hits table to long format
primers_in_Nreads_long <- primers_in_Nreads %>% 
  gather(key = Sequences, 
         value = Count, 
         "Fish1_F_Forward", "Fish1_F_Complement", "Fish1_F_Reverse", "Fish1_F_RevComp",
         "Fish1_R_Forward", "Fish1_R_Complement", "Fish1_R_Reverse", "Fish1_R_RevComp",
         "Fish2_F_Forward", "Fish2_F_Complement", "Fish2_F_Reverse", "Fish2_F_RevComp",
         "Fish2_R_Forward", "Fish2_R_Complement", "Fish2_R_Reverse", "Fish2_R_RevComp",
         "VF2_FR1d_F_Forward", "VF2_FR1d_F_Complement", "VF2_FR1d_F_Reverse", "VF2_FR1d_F_RevComp",
         "VF2_FR1d_R_Forward", "VF2_FR1d_R_Complement", "VF2_FR1d_R_Reverse", "VF2_FR1d_R_RevComp"
                  ) %>% 
  mutate(Sequences = as.factor(Sequences),
         Unique_File_name = factor(Unique_File_name,levels = sample_levels)) %>% 
  dplyr::select(-c("Read file","Stage")) %>% 
  select(where(function(x) any(!is.na(x))))
  


# PLOT 1: primers counts in readstile plot - only primers FWD & REV, foward & revcomp ----

options(scipen=10000)

library(viridis)
library(ggh4x)

#create levels for Files display on plot
Unique_File_name_Read_levels <- primers_in_Nreads_long$Unique_File_name_Read %>% unique()

#create levels for Primers display on plot

primers_tile <- primers_in_Nreads_long %>% 
  filter(Count != 0) %>% 
  ggplot2::ggplot(aes(y=Unique_File_name_Read,
                      x= Sequences ,
                      fill=(Count/`Total reads`*100),
                      group=`Primer`,alpha = 0.05)) +
  geom_tile(aes(col=`Primer`), 
            linewidth= 0.05, 
            linetype = 2)+
  geom_text(aes(label = Count),
            size=1)+
  scale_fill_gradientn(name = "Proportion of reads\n     with primer (%)",
                       colours = c("white","yellow","red","green","dark green"),
                       values = c(0,1),
                       na.value ="white") +
  scale_colour_manual(values = c("#233fdb","#ff3455")) +
  guides(color = guide_legend(override.aes = list(fill = "white", 
                                                  size = 10))) +
  theme_light(base_line_size = 0.025,
              base_size = 4) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) + 
  geom_vline(xintercept = c(4.5,8.5),color = "black") +
  xlab("Primers") +
  ylab("Amostra") +
  scale_y_discrete(limits=rev) +
  ggtitle(label = paste0("Ecomol - ",prjct_rad),
              subtitle = "Presen√ßa de primers nas amostras:\n    intensidade de cor relativa √† contagem do respectivo primer no conjunto de reads (max. mismatch = 1)") +
  theme(strip.text.y = element_text(size = 8),
        plot.title = element_text(size=8),
        plot.subtitle = element_text(size=6),
        legend.text= element_text(size=4),
        legend.title = element_text(size=6))+ scale_alpha(guide = 'none')+
  facet_grid(rows = vars(Project),scales = "free",space = "free")

primers_tile


ggsave(file = paste0(figs_path,"/",prjct_rad,"-primers_found_in_reads_1e.pdf"),
     plot = primers_tile,
     device = "pdf",
     width = 15,
     height = 45,
     units = "cm",
     dpi = 300)


#once generated, it is necessary to check if the FWD and REV primers orientation is correct.
#   if not, change the script as in:    https://benjjneb.github.io/dada2/ITS_workflow.html

# As expected, the FWD primer is found in the forward reads in its forward orientation,
#    and in some of the reverse reads in its reverse-complement orientation
#    (due to read-through when the ITS region is short).
# Similarly the REV primer is found with its expected orientations.
#
# Note: Orientation mixups are a common trip-up. If, for example,
#    the REV primer is matching the Reverse reads in its RevComp orientation,
#    then replace REV with its reverse-complement orientation
# (REV <- REV.orient[["RevComp"]]) before proceeding.
```

#### Primer removal with **_Cutadapt_** 

The **_cutadapt_** software ([DOI:10.14806/ej.17.1.200](http://journal.embnet.org/index.php/embnetjournal/article/view/200)) was used for primer removal on read sequences.


#### Generate and execute primer-specific commands

```{r eval=FALSE}
# optional: remove all primers from all reads and samples ----

sample_idx_tbl$Stage %>% unique()
sample_idx_tbl$`Read file` %>% unique()
#10 - map sample names to cutadapt reads files ----

#name outputs
cutadapt_files <- sample_idx_tbl %>% 
  filter(Stage %in% c("FWD_R1_paired", "FWD_R2_paired")) %>% 
  # mutate(`Read file` = str_replace_all(.$`Read file`,pattern = "N-cleaned|Ncleaned",replacement = "cutadapt")) %>% 
  mutate(`Read file` = str_replace_all(.$`Read file`,pattern = "/paired/",replacement = "/cutadapt/")) %>% 
  mutate(`Read file` = str_replace_all(.$`Read file`,pattern = "FWD_R1|FWD_R2",replacement = "cutadapt")) %>% 
  mutate(Stage = str_replace_all(.$Stage,pattern = "_paired",replacement = "_cutadapt")) 



cutadapt_files$`Read file` %>% unique()



sample_idx_tbl <- bind_rows(sample_idx_tbl,cutadapt_files)

names(sample_idx_tbl$`Read file`) <- sample_idx_tbl$Unique_File_name

#names of the primers that were found in reads 
(colSums(primers_in_Nreads[1:length(primers_seqs)]) == 0)
colnames(primers_in_Nreads[1:length(primers_seqs)])[(colSums(primers_in_Nreads[1:8]) != 0)] # only primer counts cols
(colnames(primers_in_Nreads[1:length(primers_seqs)]))

 
 
#create cutadapt flags from identified primers
#all -----
  primers_all_orients$Primer %>% unique()
#remove primers and filter only the reads that contain the expected primer ----
#prepare cutadapt flags specific for each primer
{

# COI_3primers_ ----
# select the respective orientations found to create cutadapt flags
 COI_3primers_FWD.orients <-  primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("_F_Forward"))]

 COI_3primers_FWD.RC <-  primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                grep(pattern = c("_F_RevComp"))]

 COI_3primers_REV.orients <-  primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                                    grep(pattern = c("_R_Forward"))]

 COI_3primers_REV.RC <-  primers_all_orients$Sequence[primers_all_orients$`Orientation name` %>%
                                               grep(pattern = c("_R_RevComp"))]

# creat flags
 # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
 COI_3primers_R1.flags <- paste("-g", COI_3primers_FWD.orients, "-a", COI_3primers_REV.RC)
 # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
 COI_3primers_R2.flags <- paste("-G", COI_3primers_REV.orients, "-A", COI_3primers_FWD.RC)

 COI_3primers_R1.flags
 COI_3primers_R2.flags

 
}






# name the vector of files to not mix samples
names(sample_idx_tbl$`Read file`) <- sample_idx_tbl$Unique_File_name

{
# # COI_3primers ----
COI_3primers_fnFs.cut <- sample_idx_tbl$`Read file`[sample_idx_tbl$Stage == "FWD_R1_cutadapt" & sample_idx_tbl$Primer == "VF2_FR1d;Fish1;Fish2"]
  
COI_3primers_fnRs.cut <- sample_idx_tbl$`Read file`[sample_idx_tbl$Stage == "FWD_R2_cutadapt" & sample_idx_tbl$Primer == "VF2_FR1d;Fish1;Fish2"]

COI_3primers_fnFs.paired <- sample_idx_tbl$`Read file`[sample_idx_tbl$Stage == "FWD_R1_paired"& sample_idx_tbl$Primer == "VF2_FR1d;Fish1;Fish2"]

COI_3primers_fnRs.paired <- sample_idx_tbl$`Read file`[sample_idx_tbl$Stage == "FWD_R2_paired"& sample_idx_tbl$Primer == "VF2_FR1d;Fish1;Fish2"]

}
  
```
  
#### Identify primer specific read files 

```{r eval=FALSE}

# Run Cutadapt

# COI_3primers ----
for(i in seq_along(COI_3primers_fnFs.cut)) {
system2(cutadapt, args = c(COI_3primers_R1.flags, COI_3primers_R2.flags, "-j 70", "-n", 3, # -n 2 required to remove FWD and REV from reads !!!!!!!!3
"-o", COI_3primers_fnFs.cut[i], "-p", COI_3primers_fnRs.cut[i], # output files
COI_3primers_fnFs.paired[i], COI_3primers_fnRs.paired[i],  # input files
"--minimum-length 10 --discard-untrimmed")) # guarantee no zerolength reads
}

```

<br>

### Identify error rates intrinsic to sequencing

```{r, eval=FALSE}
# 13 - learn error rates ----

# must be performed independently for diferent sequencing runs !

################ if cutadapted, must change input here !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
sample_idx_tbl$Stage %>% unique()

all_fnFs.paired <- sample_idx_tbl %>%
  filter(Stage %in% c("FWD_R1_cutadapt","REV_R1_cutadapt")) %>% 
  filter(file.exists(`Read file`)) %>% 
    pull(`Read file`)

all_fnRs.paired <- sample_idx_tbl %>%
  filter(Stage %in% c("FWD_R2_cutadapt","REV_R2_cutadapt")) %>% 
  filter(file.exists(`Read file`)) %>% 
    pull(`Read file`)



length(all_fnFs.paired)
length(all_fnRs.paired)



# errors in R1 reads ----
  all_errF <- learnErrors(fls = all_fnFs.paired,
                               multithread=TRUE,randomize = TRUE)
# errors in R2 reads ----
  all_errR <- learnErrors(fls = all_fnRs.paired,
                               multithread=TRUE,randomize = TRUE)

```

<br>

### Dereplication: grouping into ASVs

On this step each library is reduced to its unique composing sequences and their counts.

```{r, eval=FALSE}
# 14 - dada dereplication ----
#           !!!!!!!!!!!!!!!!!!!! must be performed separately per sequencing run
# #all ----
        all_derep_forward <- derepFastq(all_fnFs.paired, verbose=TRUE)

        all_derep_reverse <- derepFastq(all_fnRs.paired, verbose=TRUE)

        all_derep_forward[13]
        all_derep_reverse[13]


        all_dadaFs <- dada(all_derep_forward, err=all_errF, multithread=TRUE)
        all_dadaRs <- dada(all_derep_reverse, err=all_errR, multithread=TRUE)
# 
```

<br>

### Merge read pairs 

On this step the forward an reverse reads are merged, by overlap, in order to reconstruct the insert full sequence.

```{r, eval=FALSE}
# 15 - merge read pairs ----
# #merging ----
        all_mergers <- mergePairs(dadaF = all_dadaFs,
                                  derepF =  all_derep_forward,
                                  dadaR =  all_dadaRs,
                                  derepR =  all_derep_reverse,
                                  # minOverlap = 12,
                                  minOverlap = 10,
                                  maxMismatch = 1,
                                  # returnRejects = TRUE,
                                  # justConcatenate = TRUE,
                                  verbose=TRUE)
        
#combine sequence tables of different merging steps, or with concat, R1, R2 ----
 
 #must use a customized function that is not on dada2 (by benjjneb)
 
 sumSequenceTables <- function(table1, table2, ..., orderBy = "abundance") {
  # Combine passed tables into a list
  tables <- list(table1, table2)
  tables <- c(tables, list(...))
  # Validate tables
  if(!(all(sapply(tables, dada2:::is.sequence.table)))) {
    stop("At least two valid sequence tables, and no invalid objects, are expected.")
  }
  sample.names <- rownames(tables[[1]])
  for(i in seq(2, length(tables))) {
    sample.names <- c(sample.names, rownames(tables[[i]]))
  }
  seqs <- unique(c(sapply(tables, colnames), recursive=TRUE))
  sams <- unique(sample.names)
  # Make merged table
  rval <- matrix(0L, nrow=length(sams), ncol=length(seqs))
  rownames(rval) <- sams
  colnames(rval) <- seqs
  for(tab in tables) {
    rval[rownames(tab), colnames(tab)] <- rval[rownames(tab), colnames(tab)] + tab
  }
  # Order columns
  if(!is.null(orderBy)) {
    if(orderBy == "abundance") {
      rval <- rval[,order(colSums(rval), decreasing=TRUE),drop=FALSE]
    } else if(orderBy == "nsamples") {
      rval <- rval[,order(colSums(rval>0), decreasing=TRUE),drop=FALSE]
    }
  }
  rval
}


# use reads R1 and R2 separetely ----


mergers_seqtab <- makeSequenceTable(samples = all_mergers)
R1_seqtab <- makeSequenceTable(samples = all_dadaFs)
R2_seqtab <- makeSequenceTable(samples = all_dadaRs)

# FWD & REV oriented mergers
      run_mergers_FWD_seqtab <- makeSequenceTable(samples = run_mergers_FWD)
      run_mergers_REV_seqtab <- makeSequenceTable(samples = run_mergers_REV)

#aten√ß√£o. tem que inverter o R2 pra somar as tabelas aqui, antes do merge!!!!!!!!!!!
      colnames(run_mergers_REV_seqtab) <- dada2:::rc(colnames(run_mergers_REV_seqtab))


      run_mergers_ALL_seqtab <- sumSequenceTables(table1 = run_mergers_FWD_seqtab, table2 = run_mergers_REV_seqtab)
 
      
      
dada2:::is.sequence.table(mergers_seqtab)
 # dada2:::is.sequence.table(concat_seqtab)
 dada2:::is.sequence.table(R1_seqtab)
 dada2:::is.sequence.table(R2_seqtab)

 colnames(mergers_seqtab)
 # colnames(concat_seqtab)
 colnames(R1_seqtab)
 colnames(R2_seqtab)
 
# dim(concat_seqtab)
dim(mergers_seqtab)
dim(R1_seqtab)
dim(R2_seqtab)

      
      
      
# seq2 ----

# FWD & REV oriented mergers
seq2_mergers_FWD_seqtab <- makeSequenceTable(samples = seq2_mergers_FWD)
seq2_mergers_REV_seqtab <- makeSequenceTable(samples = seq2_mergers_REV)

#aten√ß√£o. tem que inverter o R2 pra somar as tabelas aqui, antes do merge!!!!!!!!!!!
      colnames(seq2_mergers_REV_seqtab) <- dada2:::rc(colnames(seq2_mergers_REV_seqtab))


      # seq2_mergers_ALL_seqtab <- sumSequenceTables(table1 = seq2_mergers_FWD_seqtab, table2 = seq2_mergers_REV_seqtab)
      seq2_mergers_ALL_seqtab <- seq2_mergers_FWD_seqtab

      
      dim(seq2_mergers_ALL_seqtab)


getN <- function(x) sum(getUniques(x))


sapply(all_mergers, getN)
sapply(all_dadaFs, getN) #only R1
sapply(all_dadaRs, getN) #only R2


mergers_seqtab <- makeSequenceTable(all_mergers)


mergers_seqtab <- sumSequenceTables(seq2_mergers_ALL_seqtab, seq3_mergers_ALL_seqtab)
mergers_seqtab <- seq2_mergers_ALL_seqtab
# mergers_seqtab <- seq3_mergers_ALL_seqtab


dada2:::is.sequence.table(mergers_seqtab)

colnames(mergers_seqtab)
 

dim(mergers_seqtab)




all_seqtab <- mergers_seqtab

# rm(all_seqtab)
# rm(all_seqtab.nochim)
rownames(all_seqtab) 
colnames(all_seqtab) %>% length()
colnames(all_seqtab) %>% unique() %>% length()

dim(all_seqtab)
str(all_seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(all_seqtab)))
table(nchar(getSequences(all_seqtab))) %>% plot() #size distribution of the ASVs

```

<br>

### Remove _chimeras_ 

_Chimeras_ are artificial read pairs that might have been generated erroneously on sequencing. The **DADA2** package estimates the probability of a sequence to be chimeric given the abundancy of its parental sequnces. After chimeric sequences removal, the remaining ASVs length distribution is assessed. On further steps it will be used to restrict analisys to ASVs compatible with each primer amplicons' length interval, in order to keep of unexpected ASVs.

```{r, eval=FALSE}
# 16 - remove chimeras ----


# any(colnames(C1conc_seqtab) %in% colnames(all_seqtab))

mergers_seqtab.nochim <- removeBimeraDenovo(mergers_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)  

R1_seqtab.nochim <- removeBimeraDenovo(R1_seqtab, method="consensus", multithread=TRUE, verbose=TRUE) 
R2_seqtab.nochim <- removeBimeraDenovo(R2_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)  




all_seqtab.nochim <- mergers_seqtab.nochim


#minFoldParentOverAbundance??
dim(all_seqtab.nochim)
sum(all_seqtab.nochim)/sum(all_seqtab) # =  0.9567811 , perda de 4.4% na abundancia -> estes 4.4% s√£o quimeras

#count proportion of ASVs of a given length
table(nchar(getSequences(all_seqtab.nochim)))
table(nchar(getSequences(all_seqtab.nochim))) %>% plot()

                                                                                                                                                                                                                                                                                                                                                                                                                                                            View(all_seqtab.nochim)
dim(all_seqtab.nochim)

```
<br>

<br>

### Classify taxonomy

On this step the ASVs identified by the **DADA2** pipeline, jointly for all libraries of each primer, are associated (or not) to any of the sequences on the Reference 12S Sequences Database. DADA2 has two strategies to identify taxa. The first, _assignSpecies_, identify perfect matches of the ASVs in the Reference Database. The second, _assignTaxonomy_, use a RDP Naive Bayesian Classifier algorithm (Wang, 2007) with kmer size 8 and 100 bootstrap replicates to associate ASVs to the Reference Database Sequences. In the latter, the taxonomy ranks classification is proportional to the sequence similarity, although this relation is not yet clear to us.


#Exact species
```{r, eval=FALSE}

#19 - classify taxonomy exactly ----

str(mergers_seqtab)

rownames(all_seqtab.nochim)

dim(mergers_seqtab)

# merges

mergers_sps <- dada2::assignSpecies(seqs = mergers_seqtab.nochim,allowMultiple = 10,
                               # refFasta =  "~/prjcts/fish_eDNA/DB/mai22/DB/LGC12Sdb-mai22-dada_SP_fullDB.fasta",
                                refFasta =  "~/prjcts/fish_eDNA/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                               tryRC=TRUE,
                               n = 20000,
                               verbose = TRUE)



      R1_sps <- dada2::assignSpecies(seqs = R1_seqtab.nochim,allowMultiple = 10,
                               # refFasta =  "~/prjcts/fish_eDNA/data/refs/db/LGC/12S_BOLD_species.fasta",
                               refFasta =  "~/prjcts/fish_eDNA/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                               tryRC=TRUE,
                               n = 20000,
                               verbose = TRUE)

      R2_sps <- dada2::assignSpecies(seqs = R2_seqtab.nochim,allowMultiple = 10,
                               # refFasta =  "~/prjcts/fish_eDNA/data/refs/db/LGC/12S_BOLD_species.fasta",
                               refFasta =  "~/prjcts/fish_eDNA/data/refs/db/BOLD/BOLD_dada_tax_Sp.fasta",
                               tryRC=TRUE,
                               n = 20000,
                               verbose = TRUE)
     
```

# Unexact species or other taxonomic ranks
```{r, eval=FALSE}
#20 - classify taxonomy ----
sample_names(all_taxa)

      mergers_taxa <- dada2::assignTaxonomy(seqs = mergers_seqtab.nochim,
                                            refFasta =  "~/prjcts/fish_eDNA/data/refs/db/LGC/12S_BOLD_tax_10ranks.fasta",

                                            multithread=TRUE, tryRC=TRUE,
                                            taxLevels = c("DB","Kingdom","Phylum","Class","Order","Family",
                                                          "Genus", "Species","Specimen","Basin"),
                                            
                           # refFasta =  "~/prjcts/fish_eDNA/DB/mai22/DB/LGC12Sdb-mai22-dada_tax_fullDB.fasta",
                           # multithread=TRUE, 
                           # tryRC=TRUE,
                           # taxLevels = c("Kingdom","Phylum","Class",
                                         # "Order", "Family", "Genus", 
                                         # "Species","Specimen","Basin"),
                           outputBootstraps = TRUE, verbose = TRUE )
      


R1_taxa <- dada2::assignTaxonomy(seqs = R1_seqtab.nochim,
                             refFasta =  "~/prjcts/fish_eDNA/data/refs/db/LGC/12S_BOLD_tax_10ranks.fasta",
                           multithread=TRUE, tryRC=TRUE,taxLevels = c("DB","Kingdom","Phylum","Class","Order","Family", "Genus", "Species","Specimen","Basin"),
                           outputBootstraps = TRUE, verbose = TRUE )



R2_taxa <- dada2::assignTaxonomy(seqs = R2_seqtab.nochim,
                          refFasta =  "~/prjcts/fish_eDNA/data/refs/db/LGC/12S_BOLD_tax_10ranks.fasta",
                           multithread=TRUE, tryRC=TRUE,taxLevels = c("DB","Kingdom","Phylum","Class","Order","Family", "Genus", "Species","Specimen","Basin"),
                           outputBootstraps = TRUE, verbose = TRUE )

R1_taxa$tax %>% View()
      
#convert dada2 exact species object to tibble
mergers_csv_sp <- mergers_sps %>% 
  as_tibble() %>% 
  mutate(ASV = rownames(mergers_sps)) %>% 
  dplyr::rename("Exact Genus (DADA2)" = "Genus",
          "exact Species (DADA2)" = "Species")
  

#convert dada2 taxonomy object to tibble
mergers_csv_taxa <- mergers_taxa$tax %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2)")) %>% 
  mutate(ASV = rownames(mergers_taxa$tax))


#adding bootstrap & exact species
mergers_csv_taxa_boot <- mergers_taxa$boot %>% 
  as_tibble() %>% 
  rename_with(.fn = ~paste0(., " (DADA2 bootstrap)")) %>% 
  mutate(ASV = rownames(mergers_taxa$boot))



# combine all
mergers_csv_IDs <- mergers_csv_taxa %>% 
  left_join(mergers_csv_taxa_boot,
            by = "ASV") %>% 
  left_join(mergers_csv_sp,
            by = "ASV") %>% 
  dplyr::select(c("ASV",
           starts_with("King"),
           starts_with("Phy"),
           starts_with("Class"),
           starts_with("Ord"),
           starts_with("Fam"),
           starts_with("Gen"),
           starts_with("Species"),
           starts_with("Specimen"),
           starts_with("Basin"),
           starts_with("Exac"),
           )) %>% 
   mutate(`Exact GenSp (DADA2)` = paste(`Exact Genus (DADA2)`,`exact Species (DADA2)`,sep=" "))
  

#Save env
   base::save.image("~/analises_eDNA/eConservation/env-_7-MiFish-08fev23.RData")

```




### Count reads and remaining ASVs

```{r, eval=FALSE}
# 17 - count reads proportion throughout the pipeline ----
getN <- function(x) sum(getUniques(x))


#denoised ----

all_dadaFs <- c(all_dadaFs)
# , 
#                 seq2_REV_dadaFs,
#                 seq3_FWD_dadaFs, 
#                 seq3_REV_dadaFs)

all_dadaRs <- c(all_dadaRs)

tbl_Denoised_FWD <- (sapply(all_dadaFs, getN) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "Denoised FWD")) %>% 
  mutate(Unique_File_name = str_remove_all(string = Unique_File_name,
                                           pattern = "-FWD|-REV"))

tbl_Denoised_REV <- (sapply(all_dadaRs, getN) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "Denoised REV")) %>% 
  mutate(Unique_File_name = str_remove_all(string = Unique_File_name,
                                           pattern = "-FWD|-REV"))

#merged  ----

tbl_Merged <- (rowSums(mergers_seqtab) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "Merged"))

#non-chimeric ----
# tbl_Non_chimeric <- (rowSums(all_seqtab.nochim) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "Non-chimeric"))
tbl_Non_chimeric <- (rowSums(R1_seqtab.nochim) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "Non-chimeric"))




# R1 and R2


colnames(R2_seqtab) <- dada2:::rc(colnames(R2_seqtab)) # reverse-complement of read2
              R1_R2_seqtab <- sumSequenceTables(table1 = R1_seqtab, table2 = R2_seqtab)
              
              dim(R1_R2_seqtab)

              

colnames(R2_seqtab.nochim) <- dada2:::rc(colnames(R2_seqtab.nochim)) # reverse-complement of read2
              R1_R2_seqtab.nochim <- sumSequenceTables(table1 = R1_seqtab, table2 = R2_seqtab)
              
              dim(R1_R2_seqtab.nochim)


              

tbl_R1_R2 <- (rowSums(R1_R2_seqtab) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "R1 + R2"))

tbl_R1_R2.nochim <- (rowSums(R1_R2_seqtab.nochim) %>% as_tibble(rownames = "Unique_File_name")) %>% `colnames<-`(c("Unique_File_name", "non-chimeric R1 + R2"))



# combine all counts by sample to plot ----

all_track <- all_filtered_out_wide %>%
  dplyr::select(-c(starts_with("Prop"))) %>%
  left_join(tbl_Denoised_FWD,by = "Unique_File_name") %>%
  left_join(tbl_Denoised_REV,by = "Unique_File_name") %>%
  left_join(tbl_Merged,by = "Unique_File_name") %>% 
  left_join(tbl_R1_R2,by = "Unique_File_name") %>% 
  left_join(tbl_R1_R2.nochim,by = "Unique_File_name") %>% 
  left_join(unique(sample_idx_tbl[c("Primer","Unique_File_name")]),by = "Unique_File_name") 



colnames(all_track) <- c("Unique_File_name",
                         # "Primer detected (pairs)", "Quality filtered (pairs)",
                         "Raw (FWD)", "Raw (REV)",
                         "Quality filtered (FWD)", "Quality filtered (REV)",
                         # "Denoised FWD (R1)", "Denoised REV (R2)", 
                         "Merged",
                         # "Non-chimeric Merged",
                         "R1 + R2",
                         "R1 + R2 non-chimeric",
                         "Primer")

all_track <- all_track %>% mutate("Total usable merged seqs/Raw reqs(%)" = (`Non-chimeric Merged`/((`Raw (FWD)` + `Raw (REV)`))*100))



all_track <- all_track %>% left_join(primers_n_samples[,c("Unique_File_name","Project","Cliente")],by = "Unique_File_name")


# Combine tables together (if there is more than one)
track_tbl <- bind_rows(all_track)



#save counts table
writexl::write_xlsx(x = all_track,
                    path = paste0(results_path,"/",prjct_rad,"-reads_and_seqs_counts",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)



colnames(track_tbl)
# transform tibble to long format, better for ggplot
  track_tbl <- track_tbl %>%
  gather(key = "Stage",
        value = "Read counts",
        "Raw (FWD)", "Raw (REV)",
        "Quality filtered (FWD)","Quality filtered (REV)",
        "Merged",
                         "R1 + R2",
                         "R1 + R2 non-chimeric",
        ) %>%
  mutate(Stage = factor(Stage, levels = c(
    "R1 + R2 non-chimeric",
                         "R1 + R2",
                         
                                          "Merged", 
                                          "Quality filtered (REV)","Quality filtered (FWD)",
                                          "Raw (REV)", "Raw (FWD)"))) 

    options(scipen = 22)
  
    
#plot samples reads/abundances
    
    
    
    

scales::show_col(viridis::viridis(n = 15))
viridis10 <- (viridis::viridis(n = 15))


  track_plot <- track_tbl %>% 
    ggplot(aes(y = Stage,x = `Read counts`, 
               fill = Stage,
               group = Unique_File_name)) +
    geom_bar(stat="identity") + 
    geom_hline(yintercept = 300000, col = 1, linetype = 2) +
    # scale_fill_manual(values = alpha(colour = viridis10[c(1,2,4,5,7,8,10,11,13,14,15,15)],
    scale_fill_manual(values = alpha(colour = rev(viridis10[c(1,2,5,7,8,11,13)]),
                                     alpha =  0.75)) +
    labs(title = paste0(prjct_rad),
         subtitle = paste0("Number ob sequences per library and Data cleaning/processing stage"
                           ),
         x = "Number of sequences",
         y = "Data processing stage") +
    # geom_label() +
    facet_wrap(~Unique_File_name, ncol = 10) +
    theme_bw(base_size = 8) +
    theme(axis.text.x = element_text(angle = 90,hjust = 0.0001,
                                     vjust = -0.00000000001,face = "bold")) +
    theme(legend.position = "bottom") +
    theme(axis.title = ggtext::element_markdown())

track_plot 

# save plot
ggsave(file = paste0(figs_path,"/",prjct_rad,"-samples_track.pdf",collapse = ""),
     plot = track_plot,
     device = "pdf",
     width = 30,
     height = 50,
     units = "cm",
     dpi = 120)

```


<br><br>

Here the **DADA2** pipeline ends.

<br><br>
   
## Phyloseq

On this step the ASVs associated to taxonomic ranks by **DADA2** and their respective counts by library, are combined using the **Phyloseq** package.

<br>

### Generate sample metadata table

Here the experiment metadata is associated to each sample.

```{r, eval=FALSE}
# 22 - create sample table ----

primers_n_samples %>% colnames()

all_samdf <- primers_n_samples[,c("Project",
                                  "Sample",
                                  "Cliente",
                                  "Unique_File_name",
                                  "Primer",
                                  "Lib",
                                  "Type",
                                  "Metadata 1", "Metadata 2", "Metadata 3", "Metadata 4", "Metadata 5", "Metadata 6","obs",
                                  "Extraction control",
                                  "PCR control","Filtration control")] %>%  
  unique() %>% as.data.frame()

samdf <- all_samdf

rownames(samdf) <- samdf$Unique_File_name
```

<br>

This sample metadata table was created with the information available for the samples analyzed on this first run. This table must be customized for each experiment.

<br><br>

### **Phyloseq** data interpretation

```{r, eval=FALSE}
#23 - interpret dada on phyloseq ----
rownames(samdf)
str(samdf)

dim(mergers_seqtab)
dim(mergers_seqtab.nochim)



mergers_ps <- phyloseq::phyloseq(phyloseq::otu_table(mergers_seqtab.nochim, taxa_are_rows = FALSE),
                                 phyloseq::sample_data(samdf),
                                 phyloseq::tax_table(mergers_taxa$tax))


R1_ps <- phyloseq::phyloseq(phyloseq::otu_table(R1_seqtab.nochim, taxa_are_rows = FALSE),
                            phyloseq::sample_data(samdf),
                            phyloseq::tax_table(R1_taxa$tax))

R2_ps <- phyloseq::phyloseq(phyloseq::otu_table(R2_seqtab.nochim, taxa_are_rows = FALSE),
                            phyloseq::sample_data(samdf),
                            phyloseq::tax_table(R2_taxa$tax))

```


<br>

### Merge and Flex Phyloseq results 

Many different graphics can be generated, together or in isolation, for all primers/libraries and taxonomic ranks.

```{r, eval=FALSE}
#24 - merge ps analisys ----
# combine all pyloseq objects in one
# by doing so, all ASVs will be combined and some will have 0 abundance
mergers_ps_tbl <- phyloseq::psmelt(mergers_ps) %>% 
  as_tibble() %>% 
  mutate(`Read origin` = "merged")  

R1_ps_tbl <- phyloseq::psmelt(R1_ps) %>% as_tibble() %>% mutate(`Read origin` = "R1") %>%  filter(Abundance >=1)
R2_ps_tbl <- phyloseq::psmelt(R2_ps) %>% as_tibble() %>% mutate(`Read origin` = "R2") %>%  filter(Abundance >=1)



#combine ps tables from all ASVs inputs
all_ps_tbl <- bind_rows(R1_ps_tbl, R2_ps_tbl,mergers_ps_tbl
                        # ,concat_ps_tbl
                        ) %>% 
  dplyr::rename("ASV" = "OTU")

# mergers_ps_tbl <- left_join(by = "ASV",x=mergers_ps_tbl,y= mergers_csv_taxa)

all_ps_tbl$OTU %>% unique()
#clear zero abundance rows
unique(mergers_ps_tbl$ASV)


mergers_ps_tbl$Sample %>%  unique()

mergers_ps_tbl$Primer %>%  unique()

#concatenate exact species table 

# only if DADA2 identification is required
# mergers_ps_tbl <- left_join(by = "ASV",  #####mudei pra parte depois de onde entra a taxonomia do blast
#                         x = mergers_ps_tbl,
#                         y = mergers_csv_IDs)

colnames(mergers_ps_tbl)


# here we would bind the tables generated for R1, R2 and concatenated ASVs, if existing
all_ps_tbl <- mergers_ps_tbl

```


#calculate sample abundances ----

```{r, eval=FALSE, echo=TRUE}
{
  all_ps_tbl <- all_ps_tbl %>%
  mutate("Relative abundance to all samples" = 0,
         "Relative abundance on sample" = 0,
         "Sample total abundance" = 0)
  abd_total <- sum(all_ps_tbl$Abundance)
  all_ps_tbl <- all_ps_tbl %>%

    dplyr::group_by(Unique_File_name,`Read origin`) %>%        #now the abundance on sample is for merged/R1/R2 separetely
    mutate("Sample total abundance" = sum(Abundance),
           "Relative abundance to all samples" = round((Abundance/abd_total*100),digits = 4),
           "Relative abundance on sample" =  round((Abundance/`Sample total abundance`*100),digits = 4)) %>%
    ungroup()

}

```

# check ASVs legths pre BLAST

```{r, eval=FALSE}

all_ps_tbl <- all_ps_tbl %>%
  mutate("ASV Size (pb)" = nchar(ASV))

# Tamanho das ASVs por amostra e Read origin ---- 
scales::show_col(viridis::viridis(n=10))


ASV_legth_by_Sample <- all_ps_tbl %>%
  mutate(Sample=factor(Sample,levels = sample_levels)) %>% 
  ggplot(aes(y=Sample,
             x=`ASV Size (pb)`,
             col = `Read origin`,
             size =`Relative abundance on sample`,
             alpha = 0.25
             )) +
  geom_jitter(height = 0.3,
              width = 0.3) +
  scale_x_continuous(breaks = c(seq(20,260,20)),
                     expand = c(0.02,0.02)) +
  scale_shape_manual(name = "Identification\n     satatus",
                                             values = c(21,4),
                                             labels=c("BLAST IDed","no ID")) +
  scale_color_manual(values = c(viridis::viridis(n=10)[c(3,6,9)]))+
  xlab("Tamanho da ASV (pb)") +
  ylab("Amostra") +
  ggtitle(label = "Ecomol - iSeq16_03112022 - piloto ",
          subtitle = "Distribution of ASVs size and Read originper Sample considering all identified ASVs") +
  theme_bw(base_size = 8) +
  theme(legend.position = "right")+
  geom_vline(xintercept = c(10,260)) +
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) 
  
ASV_legth_by_Sample
dev.off()


ggsave(file = paste0(figs_path,"/",prjct_rad,"-ASV_length_by_sample-ALL-ASVs-IDs.pdf",collapse = ""),
     plot = ASV_legth_by_Sample,
     device = "pdf",
     width = 50,
     height = 50,
     units = "cm",
     dpi = 600)




```









#BLASTn identification

```{r, eval=FALSE}
# blastn ----
## Annotate all ASVs by blastN
### select ASVs for BLASTn search ----



asvs_blast_all <- all_ps_tbl %>%
  unique() %>% 
  pull(ASV) %>% 
  unique() %>% as.character()



asvs_blast_all


length(asvs_blast_all)

library(BLASTr)
   

#COI 3 primers ----
# paralela com 2 threads ----
tictoc::tic("Parallel - Furrr 2 threads")
blast_res_1 <- BLASTr::parallel_blast(
  db_path = "/data/databases/nt/nt",
  asvs = asvs_blast_all,
  out_file = "~/analises_eDNA/eConservation/results/blast/blast_out_res_1.csv",
  out_RDS = "~/analises_eDNA/eConservation/results/blast/blast_out_res_1.RDS",
  total_cores = 80,
  perc_id = 80,
  num_threads = 2,
  perc_qcov_hsp = 80,
  num_alignments = 3,
  blast_type = "blastn"
)
tictoc::toc()# 


# #Save env
   base::save.image("~/analises_eDNA/eConservation/env-eConservation-05abr23_posBLAST-COIr1_1.RData")



saveRDS(object = blast_res,
        file = "~/analises_eDNA/eConservation/results/blast/BLAST_res-MiBird_12SV5.rds")



# blast_res1 <- readRDS(file = "~/analises_eDNA/eConservation/results/BLAST_res1.rds")



colnames(blast_res)


blast_res_full <- bind_rows(blast_res_1) %>% 
  filter(!is.na(`1_subject header`))
```


### retrieving complete taxonomies for blast res
```{r,echo=TRUE, eval=FALSE}

blast_res_full$`1_subject header` %>% unique() %>% sort()


bad_1res_IDs <- c(
  "Uncultured organism clone",
  "Uncultured prokaryote",
  "Eukaryotic synthetic construct",
  "16S rRNA amplicon fragment",
  "Uncultured Candidatus",
  "Uncultured bacterium",
  "Uncultured archaeon clone",
  "Complete Metagenome-Assembled"
  ) %>% 
  paste0(collapse = "|")


blast_res_full <- blast_res_full %>%
  mutate("blast ID" = "blast ID",
         "blast ID Origin" = "blast ID Origin",
         "query_taxID" = "query_taxID")


# pick BLASTn res IDs and mark result origin ----

for (asv in 1:nrow(blast_res_full)) {
  
  if (stringr::str_detect(string = blast_res_full$`1_subject header`[asv],pattern = bad_1res_IDs) & 
      !is.na(blast_res_full$`2_subject header`[asv])) {
    
    blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`2_subject header`[asv]),1,40)
    blast_res_full$`blast ID Origin`[asv] <- "2_"
    blast_res_full$query_taxID[asv] <- blast_res_full$`2_staxid`[asv]
    
      if (stringr::str_detect(string = blast_res_full$`2_subject header`[asv],pattern = bad_1res_IDs) & 
      !is.na(blast_res_full$`3_subject header`[asv])) {
        
        blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`3_subject header`[asv]),1,40)
        blast_res_full$`blast ID Origin`[asv] <- "3_"
        blast_res_full$query_taxID[asv] <- blast_res_full$`3_staxid`[asv]
      
            if (stringr::str_detect(string = blast_res_full$`3_subject header`[asv],pattern = bad_1res_IDs)) {
            
            blast_res_full$`blast ID`[asv] <- "Match_not_reliable"
            blast_res_full$`blast ID Origin`[asv] <- NA
            blast_res_full$query_taxID[asv] <- NA
          }
      } 
  } else {
    if (stringr::str_detect(string = blast_res_full$`1_subject header`[asv],pattern = bad_1res_IDs) & 
      is.na(blast_res_full$`2_subject header`[asv])) {
      
      blast_res_full$`blast ID`[asv] <- "Match_not_reliable"
      blast_res_full$`blast ID Origin`[asv] <- NA
      blast_res_full$query_taxID[asv] <- NA
      
      }else{
        blast_res_full$`blast ID`[asv] <- substr(as.character(blast_res_full$`1_subject header`[asv]),1,40)
        blast_res_full$`blast ID Origin`[asv] <- "1_"
        blast_res_full$query_taxID[asv] <- blast_res_full$`1_staxid`[asv]
      }
  }
}
  



blast_res_full$`blast ID` %>% unique() %>% sort()



blast_res_full$`blast ID`<-  blast_res_full$`blast ID` %>% 
  stringr::str_remove(pattern = "^  |^ |Uncultured |uncultured |Candidatus |MAG:|MAG: |MAG TPA_asm: |TPA_asm: |^Cf. |\n|candidate division ") %>% 
  stringr::str_remove(pattern = "^  |^ |Uncultured |uncultured |Candidatus |MAG:|MAG: |MAG TPA_asm: |TPA_asm: |^Cf. |\n|candidate division ") %>% 
  stringr::str_remove_all(pattern = "\\[|\\]") %>% 
  # stringr::str_remove(pattern = "\\:.*.$") %>% 
  stringr::str_replace(pattern = "cf\\. ",replacement = "") %>% 
  stringr::str_replace(pattern = "nr\\. ",replacement = "") %>% 
  stringr::str_replace(pattern = "sp\\. ",replacement = "sp\\.") %>% 
  stringr::str_replace(pattern = "\\,",replacement = "") %>% 
  stringr::str_replace(pattern = "sp\\.",replacement = "sp\\. ")


# blast_res_full_bckp2 <- blast_res_full
# blast_res_full <- blast_res_full_bckp2



blast_res_full$`blast ID` %>% unique() %>% sort()
blast_res_full$`blast ID` %>% unique() %>% sort(decreasing = T)





# selecting just the first 2 names of BLAST result
for (row in 1:nrow(blast_res_full)) {

  blast_res_full$`blast ID`[row] <- stringr::str_split_fixed(string = blast_res_full$`blast ID`[row], pattern = " ",n = 3)[1:2] %>% 
    paste0(collapse = " ")

}



blast_res_full$`blast ID` %>% unique() %>% sort()







# correct confusing labels to unify identities ----
# 
blast_res_full$`blast ID`[blast_res_full$`blast ID` %in% c("Human DNA","Eukaryotic synthetic", "Human chromosome")] <- "Homo sapiens"
  blast_res_full$`blast ID` %>% unique() %>% sort()
  
  
  blast_res_full %>% filter(`blast ID` %in% c(" ")) %>% View()
  

```




### Retrieve complete taxonomy (when possible) 

```{r,echo=TRUE, eval=FALSE}
# greate a genus colum to be able to join tax results


# blast_res_full_bckp3 <- blast_res_full


blast_res_full$`blast ID` %>% unique() %>% sort()


blast_res_full <- blast_res_full %>%
  mutate(max_tax = case_when(str_detect(`blast ID`,pattern = "PREDICTED:") ~ paste0(str_remove(`blast ID`,pattern = "PREDICTED: ")," (PREDICTED)"),
                                                                        TRUE ~  str_remove(`blast ID`,pattern = " .*$")))



unique(blast_res_full$max_tax)[unique(blast_res_full$max_tax) %in% ambiguos_tx]



 blast_res_full <- blast_res_full %>%
  relocate(`blast ID`,`blast ID Origin`,`query_taxID`,`max_tax`)




blast_res_full$max_tax %>% unique() %>% sort()




# FUNCTION to retrieve tax ranks using organism taxID ----
#test ----
 source("~/R/extract_taxonomy_taxID.R")
 
#caraaaaalhoooooooo ta comendo o primeiro caracterpq?????
 extract_taxonomy_taxID("2978354")
 
#buscando as classifica√ß√µes

  future::plan(future::multisession(),      workers = 78)
  
 taxIDs2search <- blast_res_full$query_taxID %>% unique() %>% na.omit() %>% as.character()
 
 
 taxIDs2search %>% class()
 
  taxonomy_df <- furrr::future_map_dfr(.x = taxIDs2search,
                                         .f = extract_taxonomy_taxID,
                                         .options = furrr::furrr_options(seed = TRUE))
  
  
  
  taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))]
  # 
  
  taxonomy_df1 <- furrr::future_map_dfr(.x = taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))],
                                         .f = extract_taxonomy_taxID,
                                         .options = furrr::furrr_options(seed = TRUE))
  
  
  taxonomy_df <-  bind_rows(taxonomy_df,taxonomy_df1) %>% unique()
  
  taxIDs2search[!(taxIDs2search %in% c(unique(taxonomy_df$query_taxID)))]
  
  
  #Repeat until all taxIDs are found
  
  
  
  
  
  
  # taxonomy_df_bckp <- taxonomy_df
  
  
  taxonomy_df <- taxonomy_df %>% 
    filter(!Rank %in% c("no rank","clade"))


taxonomy_tbl <- taxonomy_df %>% 
  # select(-c("TaxId","Sci_name")) %>%
  dplyr::select(-c("TaxId")) %>%
  unique() %>%
  # dplyr::filter(Rank %in% c("kingdom","phylum","class","order","family")) %>%
  filter(Rank %in% c("superkingdom","kingdom","phylum","subphylum","class","subclass","order","suborder","family","subfamily","genus")) %>% 
    tidyr::pivot_wider(
      id_cols = c(query_taxID,Sci_name),
                       names_from = Rank,
                       values_from = c(ScientificName)) %>%
    # tidyr::pivot_wider(names_from = Rank,values_from = c(ScientificName,TaxId)) %>%
    # dplyr::select(max_tax,dplyr::starts_with("Scie")) %>% 
  relocate("Sci_name","query_taxID","superkingdom","kingdom","phylum","subphylum","class","subclass","order","suborder","family","subfamily","genus")



saveRDS(object = taxonomy_tbl,
        file = paste0(results_path,"/taxonomy_df_from_taxIDs.rds"))



taxonomy_tbl <- readRDS(file = paste0(results_path,"/taxonomy_df_from_taxIDs.rds"))







# complete taxonomy tbl missing ranks



# taxonomy_tbl_bckp <- taxonomy_tbl
# orgs_tbl_bckp <- orgs_tbl


taxonomy_tbl %>% colnames() %>% paste0(collapse = "\n") %>% cat()
#fill NA tax with combination of max_tax and rank
for (line in 1:nrow(taxonomy_tbl)) {
  # if (taxonomy_tbl$genus[line] %in% c("NA",NA,"")) {
  if (taxonomy_tbl$superkingdom[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$superkingdom[line] <- paste0("superkingdom of ", taxonomy_tbl$kingdom[line]) }
  
  if (taxonomy_tbl$kingdom[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$kingdom[line] <- paste0("kingdom of ", taxonomy_tbl$superkingdom[line]) }
  
  if (taxonomy_tbl$phylum[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$phylum[line] <- paste0("phylum of ", taxonomy_tbl$kingdom[line]) }
  
  if (taxonomy_tbl$subphylum[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subphylum[line] <- paste0("subphylum of ", taxonomy_tbl$phylum[line]) }
  
  if (taxonomy_tbl$class[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$class[line] <- paste0("class of ", taxonomy_tbl$subphylum[line]) }
  
  if (taxonomy_tbl$subclass[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subclass[line] <- paste0("subclass of ", taxonomy_tbl$class[line]) }
  
  if (taxonomy_tbl$order[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$order[line] <- paste0("order of ", taxonomy_tbl$subclass[line]) }
  
  if (taxonomy_tbl$suborder[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$suborder[line] <- paste0("suborder of ", taxonomy_tbl$order[line]) }
  
  if (taxonomy_tbl$family[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$family[line] <- paste0("family of ", taxonomy_tbl$suborder[line]) }
  
  if (taxonomy_tbl$subfamily[line] %in% c("NA",NA,"")) {
    taxonomy_tbl$subfamily[line] <- paste0("subfamily of ", taxonomy_tbl$family[line]) }
  
  if (is.na(taxonomy_tbl$genus[line])) {
    taxonomy_tbl$genus[line] <- paste0("genus of ", taxonomy_tbl$subfamily[line]) }
# 

}


taxonomy_tbl %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()

taxonomy_tbl <- taxonomy_tbl %>% 
  dplyr::rename(
    "Superkingdom (BLASTn)" = "superkingdom",
    "Kingdom (BLASTn)" = "kingdom",
    "Phylum (BLASTn)" = "phylum",
    "Subphylum (BLASTn)" = "subphylum",
    "Class (BLASTn)" = "class",
    "Subclass (BLASTn)" = "subclass",
    "Order (BLASTn)" = "order",
    "Suborder (BLASTn)" = "suborder",
    "Family (BLASTn)" = "family",
    "Subfamily (BLASTn)" = "subfamily",
    "Genus (BLASTn)" = "genus")







# taxonomy_tbl_bckp2 <- taxonomy_tbl

#10- bind tax rank cols to DB_tbl ----
blast_res_tax <- left_join(x = blast_res_full, 
                           y = taxonomy_tbl,
                           by = "query_taxID")



blast_res_tax[is.na(blast_res_tax$`Superkingdom (BLASTn)`),] %>% View()




all_ps_tbl %>% unique()

blast_res_tax %>% filter(`Genus (BLASTn)` %in% c(NA)) %>% View()



blast_res_full %>% colnames()



# saveRDS(object = blast_res_tax,file = "~/refs/COIr1_blast_res_tax_27969seqs.rds")
saveRDS(object = blast_res_tax,file = "~/COIr1_blast_res_tax_25434seqs.rds")

```

#combine BLAST and DADA2 results ----

```{r, eval=FALSE, echo=TRUE}


colnames(blast_res_tax)[colnames(blast_res_tax) == "OTU"] <- "ASV"



all_ps_tbl_blast <- left_join(x = all_ps_tbl,y = blast_res_tax,by = "ASV")



# all_ps_tbl_blast_bckp <- all_ps_tbl_blast
# all_ps_tbl_blast <- all_ps_tbl_blast_bckp


``` 


# Remove uninformative columns from complete results table

```{r,echo=TRUE, eval=FALSE}
# If the identifications did not use DADA2 results, remove corresponding columns to make files and tables lighter


all_ps_tbl_blast %>% colnames() %>% paste0(collapse = '",\n"') %>% cat()



# add DADA results: MiFish primers only ----

all_ps_tbl_blast <- all_ps_tbl_blast %>%
  dplyr::select(-c(
    # "DB",
    "Kingdom",
    "Phylum",
    "Class",
    "Order",
    "Family",
    "Genus",
    "Species",
    "Specimen",
    "Basin",
    # "Read origin",
    # # "(DADA2 bootstrap)DB",
    # "(DADA2 bootstrap)Kingdom",
    # "(DADA2 bootstrap)Phylum",
    # "(DADA2 bootstrap)Class",
    # "(DADA2 bootstrap)Order",
    # "(DADA2 bootstrap)Family",
    # "(DADA2 bootstrap)Genus",
    # "(DADA2 bootstrap)Species",
    # "(DADA2 bootstrap)Specimen",
    # "(DADA2 bootstrap)Basin",
    # "exact Genus",
    # "exact Species"
    ))


# novo local de entrada da taxonomia do DADA2
all_ps_tbl_blast <- all_ps_tbl_blast %>%
  left_join(y = mergers_csv_IDs, by = "ASV")



``` 




# Final ID 

```{r,echo=TRUE, eval=FALSE}

#all_ps_tbl_blast_bckp2 <- all_ps_tbl_blast
#all_ps_tbl_blast <- all_ps_tbl_blast_bckp2 

# DADA2 final identification ----


#NOW INCLUDING FAMILY
all_ps_tbl_blast <- all_ps_tbl_blast %>%
  mutate("Final ID (DADA2)" = if_else((`exact Species (DADA2)` %in% c(NA,"NA", "NA NA")),
                              if_else((`Species (DADA2)` %in% c(NA,"NA")),
                                      if_else(`Genus (DADA2)` %in% c(NA,"NA"),
                                              # if_else((`blast ID` %in% c(NA,"NA")),
                                                      # if_else((Subfamily %in% c(NA,"NA")),
                                                              if_else((`Family (DADA2)` %in% c(NA,"NA")),
                                                                      # if_else((Suborder %in% c(NA,"NA")),
                                                                              if_else((`Order (DADA2)` %in% c(NA,"NA")),
                                                                                      `Class (DADA2)`,
                                                                                      `Order (DADA2)`),
                                                                              # Suborder),
                                                                      `Family (DADA2)`),
                                                              # Subfamily),
                                                     # `blast ID`),
                                              `Genus (DADA2)`),
                                      `Species (DADA2)`),
                              as.character(`Exact GenSp (DADA2)`)))

# BLASTn final identification ----



all_ps_tbl_blast <- all_ps_tbl_blast %>%
           mutate("Final ID (BLASTn)" = `blast ID`)



all_ps_tbl_blast$`Final ID (BLASTn)` %>% unique() %>%  sort()


colnames(all_ps_tbl_blast)[colnames(all_ps_tbl_blast) == "ASV"] <- "ASV (Sequence)"
# names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "ASV Size (pb)"

```

### ASVs seqs

```{r,echo=TRUE, eval=FALSE}
#25 - recover all ASVs sequences to prepare fasta ----

#all ----
# giving our seq headers more manageable names (ASV_1, ASV_2...)
all_asv_seqs <- tibble("ASV (Sequence)" = unique(all_ps_tbl_blast$`ASV (Sequence)`))

all_asv_seqs <- all_asv_seqs %>%
  mutate("ASV length" = nchar(`ASV (Sequence)`),
  # mutate("ASV length" = nchar(unfactor(ASV)),
         "ASV header" = as.character(""))

all_asv_seqs <- all_asv_seqs[rev(base::order(all_asv_seqs$`ASV length`)),]

all_asv_seqs$`ASV (Sequence)` %>% unique()


all_asv_seqs <- all_asv_seqs %>% 
  mutate(`ASV header` = paste0(">ASV_",row_number(),"_",`ASV length`, "bp"))

#combine ASV headers and all_ps_tbl
all_ps_tbl_blast <- dplyr::left_join(x = all_ps_tbl_blast,
                                     y = all_asv_seqs[,c(1,3)],
                                     by = "ASV (Sequence)")

# making and writing out a fasta of our final ASV seqs with tax


all_asv_seqs <- all_ps_tbl_blast %>% 
  dplyr::select(c("ASV (Sequence)", "Primer",
           "Class (BLASTn)", "Family (BLASTn)", "Genus (BLASTn)", "blast ID")) %>% 
  unique() %>% 
  right_join(all_asv_seqs,by = "ASV (Sequence)") %>% 
  group_by(`ASV (Sequence)`,`ASV header`) %>% 
  
  mutate(`ASV header` = paste0(c(`ASV header`,`Primer`, `Class (BLASTn)`, `Family (BLASTn)`, `Genus (BLASTn)`, `blast ID`),collapse = "|")) %>%
  ungroup() %>% 
  unique() 


#write fasta file with ASVs and Taxonomy
all_asv_fasta <- c(rbind(all_asv_seqs$`ASV header`, all_asv_seqs$`ASV (Sequence)`))

write(all_asv_fasta, paste0(results_path,"/",prjct_rad,"-all_ASVs_COIr1.fasta"))



log10(all_ps_tbl_blast$Abundance) %>% table()  %>%  plot()
log10(all_ps_tbl_blast$`Relative abundance on sample`) %>% table()  %>%  plot()
(all_ps_tbl_blast$`Relative abundance on sample`) %>% table()  %>%  plot()


all_ps_tbl_blast %>% unique()
all_ps_tbl_blast %>% duplicated()
all_ps_tbl_blast[all_ps_tbl_blast %>% duplicated(),]



all_ps_tbl_blast$Abundance %>% table()  %>%  plot()
all_ps_tbl_blast$`Relative abundance on sample` %>% table() %>%  plot()
all_ps_tbl_blast$`ASV Size (pb)` %>% table() %>%  plot()

```

###SWARM - ASVs to OTUs

```{r,echo=TRUE, eval=FALSE}


asvs_abd <- all_ps_tbl_blast %>%
  dplyr::select(c("ASV (Sequence)","ASV header","Abundance")) %>% 
  group_by(`ASV (Sequence)`,`ASV header`) %>%
  mutate("ASV total abundance" = sum(Abundance)) %>%
  dplyr::select(c(`ASV (Sequence)`,`ASV header`,`ASV total abundance`)) %>%
  unique() %>%
  mutate(`ASV header abd` = paste0(`ASV header`,"_",`ASV total abundance`))

#write fasta file with ASVs and Taxonomy
all_asv_fasta_abd <- c(rbind(asvs_abd$`ASV header abd`, asvs_abd$`ASV (Sequence)`))

write(all_asv_fasta_abd, paste0(results_path,"/",prjct_rad,"-ASVs_abd.fasta"))

paste0(results_path,"/",prjct_rad,"-ASVs_abd.fasta")




```

#### Run SWARM V2 on command line

```{r ,echo=TRUE, eval=FALSE}
# 1 - move to swarm folder

cd $PRJCT_DIR/results/swarm

# 2 - run SWARM

# swarm -t 50 ~/analises_eDNA/eConservation/results/eConservation-ASVs_abd.fasta -s eConservation_abd-swarm.stats -o eConservation_abd-swarm.out -w eConservation_abd-representative_OTUs.fasta -i eConservation_abd-swarm.structure -f


swarm_clust <- list.files(path = swarm_path,
                          pattern = "swarm.out",
                          full.names = TRUE ) %>% 
  readr::read_lines()

 

find_otu <- function(ASV_header, clusters_swarm){
  
  ASV_OTU_tbl <- tibble::tibble(`ASV header abd` = stringr::str_remove_all(string = ASV_header,
  pattern  = ">"),
                                OTU = 0)
  
  ASV_OTU_tbl$OTU <- which(grepl(x = clusters_swarm,
                                     pattern = ASV_OTU_tbl$`ASV header abd`))
                                     
 return(ASV_OTU_tbl)
} 


# Vers√µes paralelas
cores_to_be_used <- future::availableCores() - 2 # Usar todos os cores -2 = 78
future::plan(future::multisession(workers = cores_to_be_used))




ASVs_and_OTUs <- furrr::future_map_dfr(.x = asvs_abd$`ASV header abd`,
                                       clusters_swarm = swarm_clust,
                                       .f = find_otu,
                                       .options = furrr::furrr_options(seed = NULL))



ASVs_and_OTUs$`ASV header abd` <- ASVs_and_OTUs$`ASV header abd` %>% str_replace(pattern = "^",replacement = ">")

ASVs_and_OTUs$OTU %>% unique() %>% length()


# asvs_abd <- asvs_abd %>% dplyr::select(-c("OTU.x", "OTU.y"))


asvs_abd <- left_join(asvs_abd,
                        ASVs_and_OTUs,
                      by = "ASV header abd")



asvs_abd

all_ps_tbl_blast <- left_join(all_ps_tbl_blast,asvs_abd[,c(1,5)],
                              by = "ASV (Sequence)")


names(all_ps_tbl_blast)[which(names(all_ps_tbl_blast)== "ASV length")] <- "Size (pb)"

colnames(all_ps_tbl_blast)

all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU) %>% View() 
all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU,`ASV length`) %>% unique() %>% View() 



all_ps_tbl_blast %>% dplyr::select(`Final ID (BLASTn)`,OTU) %>% dplyr::select(OTU) %>% unique() 
all_ps_tbl_blast %>% dplyr::select(`ASV (Sequence)`,`Final ID (BLASTn)`,OTU) %>% unique() 


```


#Identify ASVs present on the Blanks/Negative controls

```{r,echo=TRUE, eval=FALSE}

#corrigindo que na tabela os controles n√£o foram importados

all_ps_tbl_blast$Sample %>% unique()
all_ps_tbl_blast %>% unique()



all_ps_tbl_blast <- all_ps_tbl_blast %>% mutate("Remove" = "ASV exclusive to samples")

all_ps_tbl_blast$Type %>% unique()
all_ps_tbl_blast %>% colnames()

# Identify contamination based on respective controls----

all_ps_tbl_blast$Unique_File_name %>% unique() %>% sort()

all_ps_tbl_blast$Remove[(all_ps_tbl_blast$Type %in% c("Extraction control",
                                                      "PCR control",
                                                      "PCR Control",
                                                      "Filtration control",
                                                      "Negative Control",
                                                      "Positive Control",
                                                      "Control" ))] <- "Controls"


#create table with controls only 


all_contam_ASVs <- all_ps_tbl_blast %>%
  filter(Abundance > 0) %>% 
  filter(Remove %in% "Controls") %>%
  group_by(`ASV (Sequence)`, Unique_File_name) %>% 
  mutate("Max. ASV abd. in control" = max(`Relative abundance on sample`)) %>%
  ungroup() %>%
  dplyr::select("Unique_File_name","ASV (Sequence)","Max. ASV abd. in control","Final ID (BLASTn)") %>%
  unique()




# Is there any of the ASVs in control also in the Samples?

all_contam_ASVs$`ASV (Sequence)` %in% (all_ps_tbl_blast$`ASV (Sequence)` %>% unique())
all_ps_tbl_blast[((all_ps_tbl_blast$`ASV (Sequence)`) %in% all_contam_ASVs$`ASV (Sequence)`),] %>% View()



all_ps_tbl_blast$Type %>% unique()



# save new complete table to edit controls
all_ps_tbl_blast_controls <- all_ps_tbl_blast




#mark contam ASVs in all other samples

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("Prop. to PCR control" = 0,
         "Prop. to Ext control" = 0,
         "Prop. to Filt control" = 0)

# compare sample table with control table and assign foldchanges

all_ps_tbl_blast_controls[all_ps_tbl_blast_controls$`ASV (Sequence)` %in% (all_contam_ASVs$`ASV (Sequence)`),] %>% View()


all_ps_tbl_blast_controls$PCR.control <- all_ps_tbl_blast_controls$PCR.control %>% str_replace_all(pattern = ",",replacement = ";")



#confira os nomes dos controles!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
all_contam_ASVs$Unique_File_name

all_ps_tbl_blast_controls$PCR.control %>% unique()



for (line in 1:nrow(all_ps_tbl_blast_controls)) {
  
  if(all_ps_tbl_blast_controls$`ASV (Sequence)`[line] %in% (all_contam_ASVs$`ASV (Sequence)`)){
  # tab_size <- nrow(all_ps_tbl_blast_controls) 
  # line <- 788
  seq2search <- all_ps_tbl_blast_controls$`ASV (Sequence)`[line]
  file2search <- all_ps_tbl_blast_controls$Unique_File_name[line]
  
  
    PCRcontrols2search <- all_ps_tbl_blast_controls$`PCR.control`[line] %>% str_split(pattern = ";") %>% unlist()

    FILTcontrols2search <- all_ps_tbl_blast_controls$`Filtration.control`[line] %>% str_split(pattern = ";") %>% unlist()

    EXTcontrols2search <- all_ps_tbl_blast_controls$`Extraction.control`[line] %>% str_split(pattern = ";") %>% unlist()
  
  
  PCR_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% PCRcontrols2search & `ASV (Sequence)` %in% seq2search)
  FILT_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% FILTcontrols2search & `ASV (Sequence)` %in% seq2search)
  EXT_control_tbl <- all_contam_ASVs %>% filter(Unique_File_name %in% EXTcontrols2search & `ASV (Sequence)` %in% seq2search)
  
  #proportion on PCR.control
  all_ps_tbl_blast_controls$`Prop. to PCR control`[line] <- gtools::foldchange(denom = max(PCR_control_tbl$`Max. ASV abd. in control`),
                                                                         num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
  
  #proportion on Filt.control
  all_ps_tbl_blast_controls$`Prop. to Filt control`[line] <- gtools::foldchange(denom = max(FILT_control_tbl$`Max. ASV abd. in control`),
                                                                         num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
  
  #proportion on Extraction.control
  all_ps_tbl_blast_controls$`Prop. to Ext control`[line] <- gtools::foldchange(denom = max(EXT_control_tbl$`Max. ASV abd. in control`),
                                                                         num = all_ps_tbl_blast_controls$`Relative abundance on sample`[line])
  }
  #denominador = abd in.control
  #numerador = abd in sample
  # se +, amostra mais abundante que o.controle
  
  # if (line*3==) {
  #   
  # }
  }


all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% mutate("Possible contamination" = "True detection")


#mark possible contaminations
for (line in 1:nrow(all_ps_tbl_blast_controls)) {

  if ((all_ps_tbl_blast_controls$`Prop. to PCR control`[line] != 0) |
      (all_ps_tbl_blast_controls$`Prop. to Filt control`[line] != 0) |
      (all_ps_tbl_blast_controls$`Prop. to Ext control`[line] != 0) ) {
    
    all_ps_tbl_blast_controls$`Possible contamination`[line]  <-  "Possible contamination"
    
  }
  }






all_ps_tbl_blast_controls$`Prop. to PCR control` %>% table() %>% plot()



all_ps_tbl_blast_controls %>% 
  filter(`Possible contamination` %in% c("Possible contamination")) %>% View()


all_ps_tbl_blast$`ASV (Sequence)` %>%  unique()
all_ps_tbl_blast$Sample %>%  table()
all_ps_tbl_blast$Remove %>%  table()

```


## plot identified ASVs

```{r, eval=FALSE}

all_ps_tbl_blast_controls$`Superkingdom (BLASTn)` %>% unique()


library(ggtext)

ASV_legth_by_Sample_BLAST <- all_ps_tbl_blast_controls  %>%
  mutate(Sample=factor(Sample,levels = sample_levels)) %>% 
  mutate("Blast pseudo-score" = (`1_indentity`*`1_qcovhsp`/100)) %>%
  arrange(desc(`Blast pseudo-score`)) %>% 
  arrange(desc(is.na(`Blast pseudo-score`))) %>% 
  ggplot(aes(y=Sample,
             x=`ASV Size (pb)`,
             fill = `Blast pseudo-score`,
             col = `Blast pseudo-score`,
             shape = `Superkingdom (BLASTn)`,
             size =`Relative abundance on sample`,
             alpha = 0.2,
             group = `Final ID (BLASTn)`
             )) +
  geom_jitter(height = 0.3,
              width = 0.5) +
    scale_fill_gradientn(name = "BLASTn\nidentification\n _pseudo-score_ (%)",
                         na.value = "grey95",
                       colours = c("dark red","red","yellow","green","dark green"),
                       values = c(0,1),
                       breaks = c(60,65,70,75,80,85,90,95,100))+
    scale_color_gradientn(name = "BLASTn\nidentification\n _pseudo-score_ (%)",
                         na.value = "grey80",
                       colours = c("dark red","red","yellow","green","dark green"),
                       values = c(0,1),
                       breaks = c(60,65,70,75,80,85,90,95,100))+
  scale_size_continuous(name = "Abund√¢ncia\n     relativa\nna amostra (%)",
                        breaks = c(0,1,10,20,30,40,50,60,70,80,90,100),
                        ) +
  scale_x_continuous(breaks = c(seq(0,260,10)),expand = c(0.02,0.02),
                     sec.axis = dup_axis()) +
  scale_shape_manual(name = "Superkingdom (BLASTn)",
                     values = c(24,21,23),na.value = 22
                     ) +
  xlab("Tamanho da ASV (pb)") +
  ylab("Amostra") +
  ggtitle(label = paste0(prjct_rad),
          subtitle = "Status de identifica√ß√£o de todas ASVs encontradas na an√°lise") +
  theme_bw(base_size = 8) +
  theme(legend.position = "right")+
  theme(axis.text.x = element_text(angle = 0,hjust = 0.5),
        legend.title = element_markdown()
        ) +
  guides(alpha="none",
         color="none") +
  theme(strip.text = element_text(size = 14),
        axis.title = element_text(size = 14),
        axis.text.x = element_text(size = 10)) +
  facet_grid(rows = vars(Project,Cliente),
             space = "free_y",
             scales = "free_y")


ASV_legth_by_Sample_BLAST


ggsave(file = paste0(figs_path,"/",prjct_rad,"-ASV_length_by_sample-ALL-ASVs.pdf",collapse = ""),
     plot = ASV_legth_by_Sample_BLAST,
     device = "pdf",
     width = 70,
     height = 70,
     units = "cm",
     dpi = 200)


library(plotly)

ASV_legth_by_Sample_BLAST_plotly <- ASV_legth_by_Sample_BLAST %>% ggplotly(tooltip = c("Final ID (BLASTn)",
                                                   "Sample",
                                                   "ASV Size (pb)",
                                                   "Blast pseudo-score",
                                                   "Superkingdom (BLASTn)",
                                                   "Relative abundance on sample"))


htmlwidgets::saveWidget(widget = ASV_legth_by_Sample_BLAST_plotly,
                selfcontained = TRUE,
                        file = paste0(figs_path,"/",prjct_rad,"-ASVs_BLASTn_IDed_interactive_Cliente_ICMBio.html"))

```



#ASV expected length per primer
```{r, eval=FALSE, echo=TRUE}

# all_ps_tbl_blast_controls_bckp <- all_ps_tbl_blast_controls
# all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls_bckp


# fill ranges column with expected primer insert ranges

all_ps_tbl_blast_controls$Primer %>% unique() 


all_ps_tbl_blast_controls %>% colnames()


all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
mutate(`Expected length` = "FALSE")



all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
mutate(`Expected length` = case_when((Primer %in%  "VF2_FR1d;Fish1;Fish2" & `ASV Size (pb)` %in% c(220:230)) ~ "in range",
                                     TRUE ~ "out of range")) 



all_ps_tbl_blast_controls %>% colnames() %>% sort()

```

## Set curated ID


The curated identification is obtained by manually (but programatically) correcting species based on biological scientific expertise, or species names that are uncorrect. 

```{r,echo=TRUE, eval=FALSE}

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  # mutate(`Final ID ` = unfactor(`Final ID `)) %>% 
  mutate("Curated ID" = `Final ID (BLASTn)`)


# all_ps_tbl_blast_controls$`Curated ID` <- unfactor(all_ps_tbl_blast_controls$`Curated ID`)

all_ps_tbl_blast_controls$`blast ID` %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()


all_ps_tbl_blast_controls$Remove %>% unique()


all_ps_tbl_blast_controls$`Curated ID` %>% unique() %>% sort() %>% paste0(collapse = '",\n"') %>% cat()
#modificando esp√©cies combase na discuss√£o da  imers√£o


all_ps_tbl_blast_controls %>% select(`Curated ID`,OTU,Remove) %>% unique() %>% View()
```



#Reorder table

```{r, eval=FALSE}

# all_ps_tbl_blast_controls_bckp2<- all_ps_tbl_blast_controls
# all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls_bckp2


# colnames(all_ps_tbl_blast_controls) %>% paste0(collapse = '",\n"') %>% cat()




all_ps_tbl_blast_controls <-
  all_ps_tbl_blast_controls %>%
  # mutate("Read origin" = "merged") %>%11111111111\!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  dplyr::rename("PCR control" = "PCR.control",
         "Ext. control"= "Extraction.control",
         "Filt. control" = "Filtration.control") %>%
  dplyr::select(
    # colnames(all_ps_tbl_blast_controls) [!colnames(all_ps_tbl_blast_controls) %in%
    c(
      "Cliente",
      "Primer",
                 "Sample",   
            "Project",
                 # "Predator.species",
                 "Primer",
                 "Unique_File_name",
                 "Read origin",
                 "Relative abundance to all samples",
                 "Relative abundance on sample",
                 "Sample total abundance",
                 "Abundance",
            "Metadata.1",
            "Metadata.2",
            "Metadata.3",
            "Metadata.4",
            "Metadata.5",
            "obs",
            
                 
                 "Curated ID",
                 # "Final ID ",
                 "Final ID (BLASTn)",
                                                          # "Final ID (DADA2)",
            
                 "Expected length",
                 "blast ID",
            "blast ID Origin",
                 #DADA2
                                         #         
                                         #  "Exact GenSp (DADA2)",
                                         #  "exact Species (DADA2)",
                                         #  "Exact Genus (DADA2)",
                                         #  # "Group",
                                         #  # "Group (DADA2 bootstrap)",
                                         #  "Basin (DADA2)",
                                         #  "Basin (DADA2 bootstrap)",
                                         #         "Species (DADA2)",
                                         #         "Species (DADA2 bootstrap)",
                                         #         "Specimen (DADA2)",
                                         #         "Specimen (DADA2 bootstrap)",
                                         #         "Genus (DADA2)",
                                         #         "Genus (DADA2 bootstrap)",
                                         #         "Family (DADA2)",
                                         #         "Family (DADA2 bootstrap)",
                                         #         "Order (DADA2)",
                                         #         "Order (DADA2 bootstrap)",
                                         #         "Class (DADA2)",
                                         #         "Class (DADA2 bootstrap)",
                                         #         "Phylum (DADA2)",
                                         #         "Phylum (DADA2 bootstrap)",
                                         #         "Kingdom (DADA2)",
                                         #         "Kingdom (DADA2 bootstrap)",
          #blast
                 "max_tax",
          "Genus (BLASTn)",
          "Subfamily (BLASTn)",
          "Family (BLASTn)",
          "Suborder (BLASTn)",
          "Order (BLASTn)",
          "Subclass (BLASTn)",
          "Class (BLASTn)",
          "Phylum (BLASTn)",
          "Subphylum (BLASTn)",
          "Kingdom (BLASTn)",
          "Superkingdom (BLASTn)",
          # "Kingdom (BLASTn)",
                 # "1_res",
                 "1_subject header",
                 # "1_query",
                 "1_staxid",
                 "1_subject",
                 "1_indentity",
                 "1_qcovhsp",
                 "1_length",
                 "1_mismatches",
                 "1_gaps",
                 "1_query start",
                 "1_query end",
                 "1_subject start",
                 "1_subject end",
                 "1_e-value",
                 "1_bitscore",
                 # "2_res",
                 "2_subject header",
                 "2_staxid",
                 # "2_query",
                 "2_subject",
                 "2_indentity",
                 "2_qcovhsp",
                 "2_length",
                 "2_mismatches",
                 "2_gaps",
                 "2_query start",
                 "2_query end",
                 "2_subject start",
                 "2_subject end",
                 "2_e-value",
                 "2_bitscore",
                 # "3_res",
                 "3_subject header",
                 "3_staxid",
                 # "3_query",
                 "3_subject",
                 "3_indentity",
                 "3_qcovhsp",
                 "3_length",
                 "3_mismatches",
                 "3_gaps",
                 "3_query start",
                 "3_query end",
                 "3_subject start",
                 "3_subject end",
                 "3_e-value",
                 "3_bitscore",

                 
                 "Remove",
                 "ASV Size (pb)",
                 # "Size (pb)",
                 "ASV header",
                 "ASV (Sequence)",
                 
                 "OTU",
# "Extraction control",
"PCR control",
# "Filtration control",
                 "Prop. to PCR control", 
                 "Prop. to Ext control",
                 "Prop. to Filt control",
                 "Possible contamination",
                 "Type"

                 # "Tag.pairs",
                 # "Run" 
                 ))


# all_ps_tbl_blast_controls_bckp3 <- all_ps_tbl_blast_controls

all_ps_tbl_blast_controls$`Curated ID` %>%  unique() %>% sort()






#mark Possible Metazoal IDs ----

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("Possible Metazoa" = FALSE)

all_ps_tbl_blast_controls <- all_ps_tbl_blast_controls %>% 
  mutate("Possible Metazoa" = if_else(!(stringr::str_detect(string = .$`Kingdom (BLASTn)`, pattern = "acter|Archaea|Virus|Fungi")), TRUE, FALSE,missing = NA))  %>% 
  mutate("Possible Metazoa" = if_else((stringr::str_detect(string = .$`Kingdom (BLASTn)`, pattern = "Metazoa")), TRUE, FALSE ,missing = NA))
  # %>% 
#   mutate("Possible Metazoa" = if_else((stringr::str_detect(string = .$`Phylum (BLASTn)`, pattern = "bacter|proka")), TRUE, FALSE))

# all_ps_tbl_blast_controls$`Possible Metazoa`[all_ps_tbl_blast_controls$`Kingdom (BLASTn)` %in% c("Metazoa")] <- FALSE  #o or em cima t[a meio estranho!]

all_ps_tbl_blast_controls %>% filter(`Possible Metazoa` %in% c(NA,"NA")) %>% View()
all_ps_tbl_blast_controls %>% filter(`Possible Metazoa` %in% c(FALSE)) %>% View()






```

##save complete table

```{r,echo=TRUE, eval=FALSE}

# reload metadata if needed----


all_ps_tbl_blast_controls %>% colnames()
blast_res_tax %>% colnames() %>% paste0(collapse = '","') %>% cat()
# blast_res_tax <-  blast_res_tax %>% select(-c("1_query\n        start","2_query\n        start", "3_query\n        start")) 

# all_ps_tbl_blast_controls_bckp4 <- all_ps_tbl_blast_controls


#order by abundance
colnames(all_ps_tbl_blast_controls)
unique(all_ps_tbl_blast_controls$Remove)

smp_abd_ID <- all_ps_tbl_blast_controls[rev(base::order(all_ps_tbl_blast_controls$Abundance)),] %>%
  filter(`Abundance` > 0) %>% 
  dplyr::rename(
    # "ASV (Sequence)" = "ASV (Sequence)",
    # "Sample" = "Sample",
    # "Preservation" = "Metadata 1",
    "ASV absolute abundance" = "Abundance",
    # "Sample Name" = "Sample.Name",
    # "Tag pairs" = "Tag.pairs",
  #                                                              "Exact Genus (DADA2)" = "Exact Genus (DADA2)",
  #                                                              "exact Species (DADA2)" = "exact Species (DADA2)",
  #                              "Exact Genus/Species (DADA2)" = "Exact GenSp (DADA2)",
                               # "Kingdom (DADA2 boot)" = "(DADA2 bootstrap)Kingdom",
                               # "Phylum (DADA2 boot)" = "(DADA2 bootstrap)Phylum",
                               # "Class (DADA2 boot)" = "(DADA2 bootstrap)Class",
                               # "Order (DADA2 boot)" = "(DADA2 bootstrap)Order",
                               # "Family (DADA2 boot)" = "(DADA2 bootstrap)Family",
                               # "Genus (DADA2 boot)" = "(DADA2 bootstrap)Genus",
                               # "Species (DADA2 boot)" = "(DADA2 bootstrap)Species",
  #                              # "Specimen (DADA2 boot)" = "(DADA2 bootstrap)Specimen",
  #                              # "Basin (DADA2 boot)" = "(DADA2 bootstrap)Basin",
  #                              # "Superkingdom (BLASTn)" = "Superkingdom (BLASTn)",
    "BLAST ID" = "blast ID",
    # "Exact Genus and Species (DADA2)" = "exact GenSp",
    "Final ID (BLASTn)" = "Final ID (BLASTn)",
  #                              "Final ID (DADA2)" = "Final ID (DADA2)",
    "Contamination control" = "Remove",
    "Primer expected length" = "Expected length",
    ) %>% 
    mutate("Blast pseudo-score" = (`1_indentity`*`1_qcovhsp`/100)) %>% 
  mutate("Identification" =  case_when(`Blast pseudo-score` >= 98 ~ `Final ID (BLASTn)`,
                                       `Blast pseudo-score` >= 95 & `Blast pseudo-score` < 98 ~ `Genus (BLASTn)`,
                                       `Blast pseudo-score` >= 90 & `Blast pseudo-score` < 95 ~ `Family (BLASTn)`,
                                       `Blast pseudo-score` >= 80 & `Blast pseudo-score` < 90 ~ `Order (BLASTn)`,
                                       `Blast pseudo-score` >= 60 & `Blast pseudo-score` < 80 ~ `Class (BLASTn)`),
         "Identification Max. taxonomy" =   case_when(`Blast pseudo-score` >= 98 ~ "Species",
                                       `Blast pseudo-score` >= 95 & `Blast pseudo-score` < 98 ~ "Genus",
                                       `Blast pseudo-score` >= 90 & `Blast pseudo-score` < 95 ~ "Family",
                                       `Blast pseudo-score` >= 80 & `Blast pseudo-score` < 90 ~ "Order",
                                       `Blast pseudo-score` >= 60 & `Blast pseudo-score` < 80 ~ "Class")) %>% 
  relocate("Identification","Identification Max. taxonomy")%>% 
  relocate(c(
    "Cliente",
    "Project",
    "Blast pseudo-score",
    "Identification",
    "Identification Max. taxonomy",
    "Primer",
    "Sample",
    "Primer",
    "Unique_File_name",
    "Read origin",
    "Relative abundance to all samples",
    "Relative abundance on sample",
    "Sample total abundance",
    "ASV absolute abundance",
    "Metadata.1",
    "Metadata.2",
    "Metadata.3",
    "Metadata.4",
    "Metadata.5",
    "obs",
    "Primer expected length",
    "Possible Metazoa",
    
    "Curated ID",
    "Final ID (BLASTn)",
    # "Final ID (DADA2)",
    "BLAST ID",
    "Blast pseudo-score",
    ,
    "blast ID Origin")) 

dim(smp_abd_ID)

colnames(smp_abd_ID) %>% paste0(collapse = '",\n"') %>% cat()

# smp_abd_ID %>% filter(`Relative abundance on sample` >= 0.05)

smp_abd_ID %>% 
  filter(`Read origin` %in% "R2") %>% 
  pull(`ASV (Sequence)`) %>% 
  unique()

smp_abd_ID$`Superkingdom (BLASTn)` %>% unique()
smp_abd_ID$`Kingdom (BLASTn)` %>% unique()


#save complete table with all results ----


smp_abd_ID %>% 
writexl::write_xlsx(
                    path = paste0(results_path,"/",prjct_rad,"-todas_info_da_analise_",Sys.Date(),".xlsx"),
                    # path = paste0(results_path,"/",prjct_rad,"-complete_analysis_results-_7_MiBird-",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

```


### Save ASV Vs. Samples table

```{r,echo=TRUE, eval=FALSE}

# generate ASVs Vs. Samples table from complete table ----

#function to either sum or unique by column type ----
##################################################
sum_uniq <- function(vec=vec){
  
  if (is.character(vec)==TRUE) {
    suniq <- BiocGenerics::unique(vec)
  }
  if (is.numeric(vec)==TRUE) {
    suniq <- sum(vec)
  }
  return(suniq)
}
####################################################




colnames(smp_abd_ID) %>% paste0(collapse = '",\n"') %>% cat()




# generate ASVs Vs. Samples table from complete table ----

smp_abd_ID_eco <-
smp_abd_ID %>% 
  
  filter(!Cliente %in% c("LGC")) %>%
  mutate(Identification = if_else(Identification %in% c(NA,"NA"),"NA",Identification)) %>% 
  
  dplyr::select(-c("Relative abundance to all samples", 
            "Sample total abundance",
            "ASV absolute abundance",
            # "Metadata 1","Metadata 2","Metadata 3","Metadata 4","Metadata 5","obs",
            "Curated ID",
            # "Final ID (BLASTn)",
            # "Final ID (DADA2)",
            # "Extraction.control",
            "PCR control",
            "Prop. to PCR control",
            "Prop. to Ext control",
            "Prop. to Filt control",
            "Possible contamination",
            # "Primer expected length",
            "Type")) %>% 
  pivot_wider(
    id_cols = c(
      # "Identification",
      "Final ID (BLASTn)",
      # "Final ID (DADA2)",
      "Primer","Primer","Read origin","Primer expected length",
                "Possible Metazoa",
                

                        # "Kingdom (DADA2)",
                        # "Phylum (DADA2)",
                        # "Class (DADA2)",
                        # "Order (DADA2)",
                        # "Family (DADA2)",
                        # "Genus (DADA2)",
                        # "Species (DADA2)",
                        # "Specimen (DADA2)", 
                        # "Basin (DADA2)",
                        # "Exact Genus/Species (DADA2)",
                
                
                "Superkingdom (BLASTn)",
                "Kingdom (BLASTn)",
                "Phylum (BLASTn)",
                "Subphylum (BLASTn)",
                "Class (BLASTn)",
                "Subclass (BLASTn)",
                "Order (BLASTn)",
                "Suborder (BLASTn)",
                "Family (BLASTn)",
                "Subfamily (BLASTn)",
                "Genus (BLASTn)",
                "max_tax","BLAST ID","Blast pseudo-score",
                          "1_subject header","1_subject","1_indentity","1_qcovhsp",
                          "1_length","1_mismatches","1_gaps","1_query start","1_query end",
                          "1_subject start","1_subject end","1_e-value","1_bitscore","2_subject header",
                          "2_subject","2_indentity","2_qcovhsp","2_length","2_mismatches",
                          "2_gaps","2_query start","2_query end","2_subject start","2_subject end",
                          "2_e-value","2_bitscore","3_subject header","3_subject","3_indentity",
                          "3_qcovhsp","3_length","3_mismatches","3_gaps","3_query start",
                          "3_query end","3_subject start","3_subject end","3_e-value","3_bitscore",
              "Contamination control","ASV Size (pb)","ASV header","ASV (Sequence)","OTU"),
    values_from ="Relative abundance on sample",
    values_fn = sum_uniq,
    names_from = Unique_File_name,
    names_sort = TRUE,
    names_prefix = "SAMPLE ") %>% 
  relocate(c("Primer","Primer",
             "Read origin",
                        # "Kingdom (DADA2)",
                        # "Phylum (DADA2)",
                        # "Class (DADA2)",
                        # "Order (DADA2)",
                        # "Family (DADA2)",
                        # "Genus (DADA2)",
                        # "Species (DADA2)",
                        # "Specimen (DADA2)",
                        # "Basin (DADA2)",
                        # "Exact Genus/Species (DADA2)",
             
              "Superkingdom (BLASTn)",
              "Kingdom (BLASTn)",
             "Phylum (BLASTn)",
             "Subphylum (BLASTn)",
             "Class (BLASTn)",
             "Subclass (BLASTn)",
             "Order (BLASTn)",
             "Suborder (BLASTn)",
             "Family (BLASTn)",
             "Subfamily (BLASTn)",
             "Genus (BLASTn)",
             "max_tax","BLAST ID",
             
             
             "Possible Metazoa", 
             # "Identification",
             "Final ID (BLASTn)",
            # "Final ID (DADA2)",
             "Blast pseudo-score","Primer expected length","ASV Size (pb)",
             starts_with("SAMPLE ")
             )) %>%  
  mutate_if(is.numeric , replace_na, replace = 0)

smp_abd_ID_eco %>% 
         writexl::write_xlsx(
                    path = paste0(results_path,"/",prjct_rad,"-EM124_EM125-todas_infos_wide-",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)



dim(smp_abd_ID)
dim(smp_abd_ID_eco)
dim(smp_abd_ID_eco_ID)

colnames(smp_abd_ID_eco) %>% paste0(collapse = '",\n"') %>% cat()

```


# Reload results table after curation

```{r,echo=TRUE, eval=FALSE}
curated_smp_abd_ID <- smp_abd_ID



# curated_IDs_tbl <- read.csv(file = "~/analises_eDNA/eConservationdata/eConservation-IDs_curadas.csv",check.names = F,)

colnames(smp_abd_ID_DAN)
colnames(curated_IDs_tbl)

```


# Plot ASVs and samples heatmap

```{r eval=FALSE,echo=TRUE}
scales::show_col(viridis::turbo(n=10))

options(scipen = 500,digits = 2)

library(ggh4x)

smp_abd_ID$Metadata.1 %>% unique()
smp_abd_ID$Metadata.2 %>% unique()
smp_abd_ID$Metadata.3 %>% unique()
smp_abd_ID$Metadata.4 %>% unique()
smp_abd_ID$Project %>% unique()

curated_smp_abd_ID$Metadata.3 %>% unique()

# New facet label names for supp variable
supp.labs <- c("Sem Interven√ß√£o", "Com Interven√ß√£o", "Unidade de Conserva√ß√£o")
names(supp.labs) <- c("SI", "CI", "UC")

IDs_smpls_heat <- 
curated_smp_abd_ID %>% 
  filter(Project %in% c("EM125","EM124")) %>%
  mutate("Unique_File_name" = factor(Unique_File_name, levels = sample_levels)) %>%
  filter(Type %in% c("Sample")) %>%
  filter(`Primer expected length` %in% c("in range")) %>%
  filter(`Kingdom (BLASTn)` %in% c("Metazoa")) %>%
  group_by(`Curated ID`,Unique_File_name) %>%
  mutate("Relative abundance on sample sum" = round(sum(`Relative abundance on sample`),digits = 3),
         "Num ASVs per ID" = length(unique(`ASV (Sequence)`))
         ) %>%
  filter(`Relative abundance on sample sum` > 1) %>% 
  ggplot(aes( x = Unique_File_name,
             y=`Curated ID`,
             fill=`Relative abundance on sample sum`,
             group=`Curated ID`
             )) +
  geom_tile(size=0.25,
            height = 0.75,
            width = 0.75) +
  geom_text(aes(label = round(`Relative abundance on sample sum`,digits = 1)), size = 1.5
             ) +
  scale_fill_gradientn(name = "Abund√¢ncia relativa\nna amostra (%)",
                       colours = c("white","dark red","red", "yellow","green","dark green","blue"),
                       values = c(0,1),
                       breaks = c(0.001,0.01, 0.1,1,5,10,25,50,100),
                       na.value ="white",
                       trans="log10")+
  scale_linetype_manual(values=c("solid",NA)) +
  theme_grey(base_line_size = 0.025,base_size = 8) +
  xlab("Pontos de coleta") +
  ylab("Esp√©cies") +
    scale_y_discrete(limits=rev) +
  ggtitle(label = paste0("ECOMOL - ",prjct_rad, " - ",Sys.Date()),
              subtitle = "N√∫mero de ASVs por esp√©cie: Identifica√ß√µes por BLASTn com >= 80% de similaridade") +      
    facet_grid(rows = vars(`Class (BLASTn)`,`Order (BLASTn)`,`Family (BLASTn)`),
               cols = vars(Project),
               scale = 'free',space = 'free') +# Change font size
  theme(strip.text.y = element_text(size = 14,angle = 0),
        strip.text.x = element_text(size = 12),
        plot.title = element_text(size=12),
        plot.subtitle = element_text(size=10),
        axis.text.y = element_text(size=12),
        axis.title.x =element_text(size=14), 
        axis.title.y =element_text(size=14), 
        axis.text.x = element_text(size=11,angle = 45, hjust = 1),
        legend.text= element_text(size=8),
        legend.title = element_text(size=10),
        legend.key.size = unit(2, 'cm'),
        panel.border = element_rect(colour = "#000000", fill = NA),
        panel.grid = element_line(colour = "#ffffff",size = 0.01)
        ) 

IDs_smpls_heat



# ggsave(file = paste0(figs_path,"/",prjct_rad,"--Sps_98blast_ASVs_abd005_noBAC-by_read_origin.pdf",collapse = ""),
ggsave(file = paste0(figs_path,"/",prjct_rad,"-EM124-EM125","-IDs_BLASTn_per_sample_FINAL.pdf",collapse = ""),
# ggsave(file = paste0(figs_path,"/",prjct_rad,"-Genus98_found_in_samples.pdf",collapse = ""),
# ggsave(file = paste0(figs_path,"/",prjct_rad,"-ASV_length_by_sample-ALL_ASVs.pdf",collapse = ""),
# ggsave(file = paste0(figs_path,"/",prjct_rad,"-ASV_length_by_sample-ALL-ASVs-IDs.pdf",collapse = ""),
     plot = IDs_smpls_heat,
     # plot = pg,
     device = "pdf",
     width = 70,
     height = 60,
     units = "cm",
limitsize = FALSE,
     dpi = 300)




smp_abd_ID %>% 
  filter(`Kingdom (BLASTn)` %in% c("Metazoa")) %>% 
  pull(`Phylum (BLASTn)`) %>% table() %>% plot()



### Ploting ASVs

```{r, eval=FALSE}
#28- ASVs plots by sample and species ----


scales::show_col(scales::hue_pal(c = 200, h= c(0,360))(50))
    scales::show_col(viridis::viridis(n = 9))
    scales::show_col(viridis::turbo(n =15))


scales::show_col(c())
scales::show_col(c("#440154", "#440184","#FF4A00","#ba0202","#0009DD","#007004", "#24768e", "#26a784", "#79d051", "#ff2b77"))
# colors6 <- scales::show_col(c("#440154", "#0009DD","#007004","#ba0202","#FF4A00", "#03435e"))
colors6 <-c("#440154", "#0009DD","#007004","#ba0202","#FF4A00", "#03435e")

colors4 <-c("#007004","#fbff00","#FF4A00","#ba0202")
colors5 <-c("#007004","#fbff00","#FF4A00","#ba0202")

scales::show_col(colors6)

scales::show_col(colors4)





# Tamanho das ASVs por amostra e Read origin e blast id cov ---- 

ASV_legth_by_Sample <- smp_abd_ID %>% 
  mutate(Sample=factor(Sample,levels = sample_levels)) %>% 
  ggplot(aes(y=Sample,
             x=`Size (pb)`,
             fill = `Blast pseudo-score`,
             col = `Blast pseudo-score`,
             shape = is.na(`Curated ID`),
             size =`Relative abundance on sample`, 
             alpha = 0.05
             )) +
  geom_jitter(height = 0.3,
              width = 0.3) +
                                         scale_fill_gradientn(colours = rev(viridis::plasma(256))) +
                                         scale_color_gradientn(colours = rev(viridis::plasma(256))) +          #origen`
  scale_size_continuous(name = "Abund√¢ncia\n     relativa\nna amostra (%)",
                        breaks = c(0,1,10,20,30,40,50,60,70,80,90,100),
                        ) +
  scale_x_continuous(breaks = seq(0,600,20),expand = c(0.02,0.02)) +
  scale_shape_manual(name = "Identification\n     satatus",
                     values = c(21,24),
                     labels=c("BLAST IDed","no ID")) +
  xlab("Tamanho da ASV (pb)") +
  ylab("Amostra") +
  ggtitle(label = "Ecomol - _7-MiBird",
          subtitle = "Distribui√ß√£o de tamanho e origem das ASVs encontradas por amostra\nconsiderando todas ASVs") +
  theme_bw(base_size = 8) +
  theme(legend.position = "right")+
  theme(axis.text.x = element_text(angle = 45,hjust = 1)) +
  guides(alpha="none") +
  facet_grid(Primer ~ .,scales ='free_y', space ='free_y') 


ASV_legth_by_Sample

```




 



### NMDS

```{r, eval=FALSE,echo=TRUE}
library(vegan)

colnames(smp_abd_ID)


all_IDs_NMDS_tbl <- curated_smp_abd_ID %>%
  mutate(Unique_File_name = factor(Unique_File_name, levels = sample_levels)) %>% 
  filter(`Kingdom (BLASTn)` %in% c("Metazoa")) %>% 
  filter(Primer %in% c("MiFish")) %>% 
  filter(`Primer expected length` %in% c("in range")) %>% 
  filter((Type %in% c("Sample"))) %>%   #remove control samples
  select(c(Sample,Unique_File_name,Primer,`Read origin`, 
           `Final ID (BLASTn)`,
           Metadata.1,Metadata.2,
           Metadata.3,Metadata.4,
           Metadata.5,`Relative abundance on sample`
           
           )) %>% 
  unite(col = "Sample_name_read", c(Sample, Primer, `Read origin`), remove = FALSE ) %>% 
  group_by(Sample_name_read, Sample, 
           `Final ID (BLASTn)`,
           Primer,Unique_File_name,`Read origin`,
           Metadata.1,Metadata.2,
           Metadata.3,Metadata.4,
           Metadata.5
           ) %>%
  summarise(`Relative abundance on sample (%)` = sum(`Relative abundance on sample`)) %>% 
  pivot_wider(c(Sample_name_read,Sample,Primer,Unique_File_name,`Read origin`,
           Metadata.1,Metadata.2,
           Metadata.3,Metadata.4,
           Metadata.5
                ),names_from =  `Final ID (BLASTn)` ,values_from = `Relative abundance on sample (%)`) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate("Sample number" = 0) %>% 
  ungroup()  %>% 
  select(`Sample number`, 1:(ncol(.)-1)) %>% 
  mutate(Unique_File_name = factor(Unique_File_name, levels = sample_levels))

#2- associate sample numbers to sample names ----
for (sample in 1:nrow(all_IDs_NMDS_tbl)) {
  
  all_IDs_NMDS_tbl$`Sample number`[sample] <- sample
  
}



all_IDs_NMDS_tbl[,unlist(lapply(all_IDs_NMDS_tbl, is.character), use.names = FALSE)] %>% colnames()




colnames(all_IDs_NMDS_tbl)

all_IDs_NMDS_tbl %>% select(Sample_name_read, `Sample number`) %>% unique()

#3- create data.frame of species counts: rownames are Sample numbers ----


    colnames(smp_abd_ID)
    
    all_IDs_NMDS_tbl 
    
    
    colnames(all_IDs_NMDS_tbl)
    
    all_IDs_NMDS_df <- all_IDs_NMDS_tbl %>% 
      select(base::sort(colnames(.))) %>%
      relocate(c("Primer",
                 "Sample number",
                 "Sample_name_read",
                 "Read origin", 
                 "Sample",
                 "Unique_File_name",
                 "Read origin",
                 "Metadata.1",
                 "Metadata.2",
                 "Metadata.3",
                 "Metadata.4",
                 "Metadata.5"
                 )) %>%
      as.data.frame() 



#4- name rows as Sample numbers and remove column ----
row.names(all_IDs_NMDS_df) <- all_IDs_NMDS_df$`Sample number`

all_IDs_NMDS_df %>% dim()
   

all_IDs_NMDS_tbl[,unlist(lapply(all_IDs_NMDS_tbl, is.character), use.names = FALSE)] %>% colnames()

colnames(all_IDs_NMDS_df)[1:15] 

dim(all_IDs_NMDS_df)



colnames(all_IDs_NMDS_df)[12:71] <- colnames(all_IDs_NMDS_df)[12:71] %>% str_replace_all(pattern = " ",replacement = "_")
colnames(all_IDs_NMDS_df)[12:71] <- colnames(all_IDs_NMDS_df)[12:71] %>% str_replace_all(pattern = "\\(|\\)",replacement = "")

colnames(all_IDs_NMDS_df)[12:71] %>% sort()
library(vegan)

   is.numeric(all_IDs_NMDS_df[,12:71])


        all_IDs_NMDS_df <- all_IDs_NMDS_df  %>% 
  filter_all(any_vars(. != 0))
        
        
        
        all_IDs_NMDS_df[,!sapply(all_IDs_NMDS_df, is.numeric)] %>% colnames()
        
all_ps_vegan_ord_meta <- metaMDS(veg = all_IDs_NMDS_df[,12:71], comm = all_IDs_NMDS_df[,12:71])

plot(all_ps_vegan_ord_meta, type = "t")
plot(all_ps_vegan_ord_meta, type = "p")


all_ps_vegan_ord_meta %>% str()
all_ps_vegan_ord_meta %>% summary()
all_ps_vegan_ord_meta

all_ps_vegan_ord_meta$stress


  all_ps_vegan_ord_meta$points


  
#6b- extract NMDS scores from results
  
all_vegan_meta <- (vegan::scores(all_ps_vegan_ord_meta) %>% 
                     tidyr::as_tibble(rownames = "Sample number")) %>% 
  mutate(`Sample number` = as.numeric(`Sample number`))

#7- bring NMDS scores to complete table

all_vegan_meta_tbl <- left_join(x = unique(all_IDs_NMDS_tbl[,c("Sample number", "Sample_name_read", "Sample","Primer")]),
                                y = all_vegan_meta, by = "Sample number") %>% 
  # mutate(Primer=factor(Primer,levels = c("NeoFish", "MiFish", "COI")),
         # Sample = as.factor(Sample)) %>% 
  select(-c("Sample number"))


library(ggord)
library(ggh4x)
all_IDs_NMDS_tbl$Sample



summary(all_ps_vegan_ord_meta)

all_ps_vegan_ord_meta$species


viridis::turbo(n=10) %>% 
scales::show_col()


viridis::inferno(n=12) %>% 
scales::show_col()

viridis::magma(n=10) %>% 
scales::show_col(viridis::inferno(n=15,alpha = 1))
scales::show_col(colors_4)


colors_4 <- viridis::inferno(n=12)[c(3,5,8,10)]
colors_9 <- viridis::inferno(n=9)


nmds_PLOT_ord <- ggord(all_ps_vegan_ord_meta, 
      grp_in = all_IDs_NMDS_df$Metadata.3, 
      vectyp = "dotted",
      ellipse = F,
      size = 2,
      arrow = 0.5, veccol = "dark grey",
      txt = 3,
      repel = T,
      veclsz = 0.5,
      max.overlaps = 51
      )+
  annotate(geom = "text",
           x=c(-0.5),
           y=c(1.1),
           label=c(paste0("Stress: ",format(round(all_ps_vegan_ord_meta$stress,4)))),
           size=6) +
  scale_colour_manual(name = "Amostras", values = viridis::viridis(option = "turbo",n = 6, alpha = 1))+
  ggtitle(label = "LGC - _7-MiBird",
          subtitle = "NMDS das identifica√ß√µes por BLASTn")
   
nmds_PLOT_ord

ggsave(file = paste0(figs_path,"/",prjct_rad,"-",Sys.Date(),"-NMDS-COIr1_RJ.pdf",collapse = ""),
# ggsave(file = paste0(figs_path,"/",prjct_rad,"-",Sys.Date(),"-NMDS.png",collapse = ""),
     plot = nmds_PLOT_ord,
     device = "png",
     width = 32,
     units = "cm",
     height = 30,
     dpi = 600)




writexl::write_xlsx(x = all_IDs_NMDS_df,
                    path = paste0(results_path,"/",prjct_rad,"-merged_SPs_by_samples",Sys.Date(),".xlsx"),
                    col_names = TRUE,format_headers = TRUE)

library(factoextra)
library(ggforce)

```

#References

* Callahan BJ, McMurdie PJ, Rosen MJ, Han AW, Johnson AJ, Holmes SP. *DADA2: High-resolution sample inference from Illumina amplicon data.* Nat Methods. 2016 Jul;13(7):581-3. doi: 10.1038/nmeth.3869. Epub 2016 May 23. PMID: 27214047; PMCID: PMC4927377.

* Martin M. **Cutadapt removes adapter sequences from high-throughput sequencing reads.** EMBnet.journal. 2011;17(1):10‚Äì12. doi: 10.14806/ej.17.1.200. -

* McMurdie PJ, Holmes S. *phyloseq: an R package for reproducible interactive analysis and graphics of microbiome census data.* PLoS One. 2013 Apr 22;8(4):e61217. doi: 10.1371/journal.pone.0061217. PMID: 23630581; PMCID: PMC3632530.

* R Core Team (2020). **R: A language and environment for statistical computing.** R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

* Wang Q, Garrity GM, Tiedje JM, Cole JR. *Naive Bayesian classifier for rapid assignment of rRNA sequences into the new bacterial taxonomy.* Appl Environ Microbiol. 2007 Aug;73(16):5261-7. doi: 10.1128/AEM.00062-07. Epub 2007 Jun 22. PMID: 17586664; PMCID: PMC1950982.

 
\pagebreak



**This is a partial report, intended to show the current state of analyses. Many procedures and conclusions might change as the pipeline evolves. If you notice errors/mistakes/typos, or have any suggestions, we would be glad to know. _heronoh@gmail.com_**


